{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrGbL51ffQYz",
        "outputId": "e314e3a1-bb81-4171-ae1d-13b2ff35915c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Service 1"
      ],
      "metadata": {
        "id": "RLiJnxnGfNkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import optuna\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load and preprocess data\n",
        "df = pd.read_csv(\"Service1.csv\")\n",
        "\n",
        "features = ['latency_ms', 'memory_allocated', 'cpu_allocated', 'cpu_usage_pct', 'memory_usage_pct']\n",
        "scaler = MinMaxScaler()\n",
        "df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "X = df[['latency_ms', 'memory_allocated', 'cpu_allocated', 'cpu_usage_pct', 'memory_usage_pct']].values\n",
        "y = df[['cpu_usage_pct', 'memory_usage_pct']].values\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape for LSTM [batch, seq_len, input_size]\n",
        "X_train = X_train.reshape(-1, 1, X.shape[1])\n",
        "X_test = X_test.reshape(-1, 1, X.shape[1])\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
        "\n",
        "\n",
        "# LSTM Model Definition\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.fc(out[:, -1, :])\n",
        "\n",
        "\n",
        "# Optuna objective\n",
        "def objective(trial):\n",
        "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 128)\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
        "    lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "\n",
        "    model = LSTMModel(input_size=5, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(10):  # Small number for tuning\n",
        "        for xb, yb in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluation on test set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(X_test_t)\n",
        "        mse = mean_squared_error(y_test, preds.numpy())\n",
        "    return mse\n",
        "\n",
        "\n",
        "# Run Optuna\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=30)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial)\n",
        "\n",
        "# Train final model\n",
        "best_params = study.best_params\n",
        "final_model = LSTMModel(\n",
        "    input_size=5,\n",
        "    hidden_size=best_params[\"hidden_size\"],\n",
        "    num_layers=best_params[\"num_layers\"],\n",
        "    dropout=best_params[\"dropout\"]\n",
        ")\n",
        "final_model.train()\n",
        "optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params[\"lr\"])\n",
        "criterion = nn.MSELoss()\n",
        "train_loader = DataLoader(train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
        "\n",
        "for epoch in range(30):\n",
        "    for xb, yb in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        preds = final_model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Final evaluation\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = final_model(X_test_t).numpy()\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Final Test MSE: {mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXH2REMvfL-z",
        "outputId": "988d6ba8-3795-4980-c30b-d2c06bd049d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 12:48:56,761] A new study created in memory with name: no-name-9851ac7e-3cd7-474d-b5fc-6afc92f94fda\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "[I 2025-05-05 12:49:12,088] Trial 0 finished with value: 4.4632819563094604e-05 and parameters: {'hidden_size': 104, 'num_layers': 2, 'lr': 0.0025910103807583137, 'dropout': 0.22074883140519125, 'batch_size': 64}. Best is trial 0 with value: 4.4632819563094604e-05.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "[I 2025-05-05 12:49:40,679] Trial 1 finished with value: 8.968320968409622e-05 and parameters: {'hidden_size': 112, 'num_layers': 2, 'lr': 0.004697745029063806, 'dropout': 0.3146261070785563, 'batch_size': 16}. Best is trial 0 with value: 4.4632819563094604e-05.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.43588001889059336 and num_layers=1\n",
            "  warnings.warn(\n",
            "[I 2025-05-05 12:49:47,930] Trial 2 finished with value: 1.050753221081749e-05 and parameters: {'hidden_size': 59, 'num_layers': 1, 'lr': 0.0010850773612694365, 'dropout': 0.43588001889059336, 'batch_size': 32}. Best is trial 2 with value: 1.050753221081749e-05.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "[I 2025-05-05 12:50:33,481] Trial 3 finished with value: 0.00020701757839290452 and parameters: {'hidden_size': 122, 'num_layers': 3, 'lr': 0.00021582961677067277, 'dropout': 0.2917444478543066, 'batch_size': 16}. Best is trial 2 with value: 1.050753221081749e-05.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4876358985219073 and num_layers=1\n",
            "  warnings.warn(\n",
            "[I 2025-05-05 12:50:45,814] Trial 4 finished with value: 0.0005545481712667423 and parameters: {'hidden_size': 59, 'num_layers': 1, 'lr': 0.0001346903317603108, 'dropout': 0.4876358985219073, 'batch_size': 16}. Best is trial 2 with value: 1.050753221081749e-05.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "[I 2025-05-05 12:51:10,658] Trial 5 finished with value: 0.0001546609211580962 and parameters: {'hidden_size': 110, 'num_layers': 3, 'lr': 0.0005326351249829944, 'dropout': 0.4062830224734391, 'batch_size': 32}. Best is trial 2 with value: 1.050753221081749e-05.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "[I 2025-05-05 12:51:59,785] Trial 6 finished with value: 4.8767297780195876e-05 and parameters: {'hidden_size': 123, 'num_layers': 3, 'lr': 0.0004600268024798127, 'dropout': 0.1061978210235201, 'batch_size': 16}. Best is trial 2 with value: 1.050753221081749e-05.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.20458650855442956 and num_layers=1\n",
            "  warnings.warn(\n",
            "[I 2025-05-05 12:52:03,408] Trial 7 finished with value: 0.011919202176807102 and parameters: {'hidden_size': 36, 'num_layers': 1, 'lr': 0.00016644847526169992, 'dropout': 0.20458650855442956, 'batch_size': 64}. Best is trial 2 with value: 1.050753221081749e-05.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "[I 2025-05-05 12:52:22,003] Trial 8 finished with value: 0.00023209508988329325 and parameters: {'hidden_size': 128, 'num_layers': 3, 'lr': 0.0003824517764340004, 'dropout': 0.10183082773774621, 'batch_size': 64}. Best is trial 2 with value: 1.050753221081749e-05.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "[I 2025-05-05 12:52:31,916] Trial 9 finished with value: 0.00021890487136022084 and parameters: {'hidden_size': 72, 'num_layers': 3, 'lr': 0.007946921624002741, 'dropout': 0.26098473185254367, 'batch_size': 64}. Best is trial 2 with value: 1.050753221081749e-05.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4844754619496602 and num_layers=1\n",
            "  warnings.warn(\n",
            "[I 2025-05-05 12:52:37,955] Trial 10 finished with value: 7.409227806223253e-06 and parameters: {'hidden_size': 42, 'num_layers': 1, 'lr': 0.0014725805272321113, 'dropout': 0.4844754619496602, 'batch_size': 32}. Best is trial 10 with value: 7.409227806223253e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4873566455545696 and num_layers=1\n",
            "  warnings.warn(\n",
            "[I 2025-05-05 12:52:44,540] Trial 11 finished with value: 1.0721723934846817e-05 and parameters: {'hidden_size': 41, 'num_layers': 1, 'lr': 0.001406962894268554, 'dropout': 0.4873566455545696, 'batch_size': 32}. Best is trial 10 with value: 7.409227806223253e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "[I 2025-05-05 12:52:54,355] Trial 12 finished with value: 8.18960171614054e-05 and parameters: {'hidden_size': 55, 'num_layers': 2, 'lr': 0.0010367910504923343, 'dropout': 0.3957413482225272, 'batch_size': 32}. Best is trial 10 with value: 7.409227806223253e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.41324082650560906 and num_layers=1\n",
            "  warnings.warn(\n",
            "[I 2025-05-05 12:53:02,245] Trial 13 finished with value: 2.2113356903806668e-06 and parameters: {'hidden_size': 87, 'num_layers': 1, 'lr': 0.001980054885200531, 'dropout': 0.41324082650560906, 'batch_size': 32}. Best is trial 13 with value: 2.2113356903806668e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3502724397852802 and num_layers=1\n",
            "  warnings.warn(\n",
            "[I 2025-05-05 12:53:10,743] Trial 14 finished with value: 2.3504885390915497e-06 and parameters: {'hidden_size': 93, 'num_layers': 1, 'lr': 0.002479157210469155, 'dropout': 0.3502724397852802, 'batch_size': 32}. Best is trial 13 with value: 2.2113356903806668e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.34504519977062514 and num_layers=1\n",
            "  warnings.warn(\n",
            "[I 2025-05-05 12:53:18,080] Trial 15 finished with value: 2.6364970161470964e-06 and parameters: {'hidden_size': 90, 'num_layers': 1, 'lr': 0.0027653433696932634, 'dropout': 0.34504519977062514, 'batch_size': 32}. Best is trial 13 with value: 2.2113356903806668e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "[I 2025-05-05 12:53:31,048] Trial 16 finished with value: 2.5636863152082466e-06 and parameters: {'hidden_size': 88, 'num_layers': 2, 'lr': 0.002705197009697497, 'dropout': 0.005136251941827985, 'batch_size': 32}. Best is trial 13 with value: 2.2113356903806668e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "[I 2025-05-05 12:53:43,246] Trial 17 finished with value: 4.029696557574654e-05 and parameters: {'hidden_size': 77, 'num_layers': 2, 'lr': 0.009922730125932014, 'dropout': 0.36293571092530663, 'batch_size': 32}. Best is trial 13 with value: 2.2113356903806668e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.16362393996886257 and num_layers=1\n",
            "  warnings.warn(\n",
            "[I 2025-05-05 12:53:51,938] Trial 18 finished with value: 1.0182415809127999e-06 and parameters: {'hidden_size': 100, 'num_layers': 1, 'lr': 0.004763640730565888, 'dropout': 0.16362393996886257, 'batch_size': 32}. Best is trial 18 with value: 1.0182415809127999e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.16968953519972044 and num_layers=1\n",
            "  warnings.warn(\n",
            "[I 2025-05-05 12:54:00,702] Trial 19 finished with value: 3.3054280705871528e-06 and parameters: {'hidden_size': 101, 'num_layers': 1, 'lr': 0.0056579600613596345, 'dropout': 0.16968953519972044, 'batch_size': 32}. Best is trial 18 with value: 1.0182415809127999e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.12847332825216007 and num_layers=1\n",
            "  warnings.warn(\n",
            "[I 2025-05-05 12:54:07,316] Trial 20 finished with value: 1.2173978454842344e-06 and parameters: {'hidden_size': 71, 'num_layers': 1, 'lr': 0.004305517831333388, 'dropout': 0.12847332825216007, 'batch_size': 32}. Best is trial 18 with value: 1.0182415809127999e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.13064545740900846 and num_layers=1\n",
            "  warnings.warn(\n",
            "[I 2025-05-05 12:54:14,721] Trial 21 finished with value: 4.12960938929178e-06 and parameters: {'hidden_size': 68, 'num_layers': 1, 'lr': 0.004408173675829269, 'dropout': 0.13064545740900846, 'batch_size': 32}. Best is trial 18 with value: 1.0182415809127999e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.03536113035522043 and num_layers=1\n",
            "  warnings.warn(\n",
            "[I 2025-05-05 12:54:21,403] Trial 22 finished with value: 2.0053191512888092e-06 and parameters: {'hidden_size': 80, 'num_layers': 1, 'lr': 0.003819804200944895, 'dropout': 0.03536113035522043, 'batch_size': 32}. Best is trial 18 with value: 1.0182415809127999e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.0360438272466968 and num_layers=1\n",
            "  warnings.warn(\n",
            "[I 2025-05-05 12:54:29,361] Trial 23 finished with value: 1.951948570354565e-06 and parameters: {'hidden_size': 79, 'num_layers': 1, 'lr': 0.00401641873060469, 'dropout': 0.0360438272466968, 'batch_size': 32}. Best is trial 18 with value: 1.0182415809127999e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "[I 2025-05-05 12:54:40,760] Trial 24 finished with value: 3.53689535433402e-05 and parameters: {'hidden_size': 67, 'num_layers': 2, 'lr': 0.0068338843893033544, 'dropout': 0.06022091853816357, 'batch_size': 32}. Best is trial 18 with value: 1.0182415809127999e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.16048110153503697 and num_layers=1\n",
            "  warnings.warn(\n",
            "[I 2025-05-05 12:54:48,927] Trial 25 finished with value: 1.2471791722848065e-06 and parameters: {'hidden_size': 96, 'num_layers': 1, 'lr': 0.0038381349683160836, 'dropout': 0.16048110153503697, 'batch_size': 32}. Best is trial 18 with value: 1.0182415809127999e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.17921918886948468 and num_layers=1\n",
            "  warnings.warn(\n",
            "[I 2025-05-05 12:54:56,817] Trial 26 finished with value: 1.953769690385635e-06 and parameters: {'hidden_size': 99, 'num_layers': 1, 'lr': 0.00666075857193749, 'dropout': 0.17921918886948468, 'batch_size': 32}. Best is trial 18 with value: 1.0182415809127999e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "[I 2025-05-05 12:55:13,649] Trial 27 finished with value: 3.832144947364255e-05 and parameters: {'hidden_size': 113, 'num_layers': 2, 'lr': 0.009327575363832457, 'dropout': 0.14224978711933176, 'batch_size': 32}. Best is trial 18 with value: 1.0182415809127999e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.22485830213703273 and num_layers=1\n",
            "  warnings.warn(\n",
            "[I 2025-05-05 12:55:28,051] Trial 28 finished with value: 4.442415225579312e-06 and parameters: {'hidden_size': 96, 'num_layers': 1, 'lr': 0.0007621914173903908, 'dropout': 0.22485830213703273, 'batch_size': 16}. Best is trial 18 with value: 1.0182415809127999e-06.\n",
            "<ipython-input-2-c4cd4eedfe1b>:54: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "[I 2025-05-05 12:55:36,930] Trial 29 finished with value: 2.2276793322249325e-05 and parameters: {'hidden_size': 107, 'num_layers': 2, 'lr': 0.0018225756884519726, 'dropout': 0.08305350648411901, 'batch_size': 64}. Best is trial 18 with value: 1.0182415809127999e-06.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.16362393996886257 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial:\n",
            "FrozenTrial(number=18, state=1, values=[1.0182415809127999e-06], datetime_start=datetime.datetime(2025, 5, 5, 12, 53, 43, 247670), datetime_complete=datetime.datetime(2025, 5, 5, 12, 53, 51, 937712), params={'hidden_size': 100, 'num_layers': 1, 'lr': 0.004763640730565888, 'dropout': 0.16362393996886257, 'batch_size': 32}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'hidden_size': IntDistribution(high=128, log=False, low=32, step=1), 'num_layers': IntDistribution(high=3, log=False, low=1, step=1), 'lr': FloatDistribution(high=0.01, log=True, low=0.0001, step=None), 'dropout': FloatDistribution(high=0.5, log=False, low=0.0, step=None), 'batch_size': CategoricalDistribution(choices=(16, 32, 64))}, trial_id=18, value=None)\n",
            "Final Test MSE: 0.0000\n"
          ]
        }
      ]
    }
  ]
}