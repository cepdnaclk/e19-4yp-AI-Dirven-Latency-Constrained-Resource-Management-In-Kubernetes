{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Service 1"
      ],
      "metadata": {
        "id": "oxLlTTpmntxm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m39O1GpnqCw",
        "outputId": "fbeef968-070d-4a64-8f39-117d357bd053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10, Loss: 0.0017, Epsilon: 0.95\n",
            "Episode 20, Loss: 0.0021, Epsilon: 0.90\n",
            "Episode 30, Loss: 0.0009, Epsilon: 0.86\n",
            "Episode 40, Loss: 0.0006, Epsilon: 0.81\n",
            "Episode 50, Loss: 0.0003, Epsilon: 0.77\n",
            "Episode 60, Loss: 0.0003, Epsilon: 0.74\n",
            "Episode 70, Loss: 0.0006, Epsilon: 0.70\n",
            "Episode 80, Loss: 0.0006, Epsilon: 0.67\n",
            "Episode 90, Loss: 0.0003, Epsilon: 0.63\n",
            "\n",
            "Average Test Reward: -0.0456 (Higher is better)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import deque\n",
        "\n",
        "# Load and preprocess data\n",
        "df = pd.read_csv(\"Service1.csv\")\n",
        "\n",
        "features = ['latency_ms', 'cpu_usage_pct', 'memory_usage_pct', 'cpu_allocated', 'memory_allocated']\n",
        "scaler = MinMaxScaler()\n",
        "df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "X = df[features].values\n",
        "cpu_usage = df['cpu_usage_pct'].values\n",
        "mem_usage = df['memory_usage_pct'].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, cpu_train, cpu_test, mem_train, mem_test = train_test_split(\n",
        "    X, cpu_usage, mem_usage, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Discrete action space\n",
        "action_space = [\n",
        "    (-1, -1), (-1, 0), (-1, 1),\n",
        "    (0, -1),  (0, 0),  (0, 1),\n",
        "    (1, -1),  (1, 0),  (1, 1)\n",
        "]\n",
        "n_actions = len(action_space)\n",
        "\n",
        "# Q-Network definition\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Initialize everything\n",
        "memory = deque(maxlen=10000)\n",
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_decay = 0.995\n",
        "epsilon_min = 0.1\n",
        "batch_size = 64\n",
        "lr = 1e-3\n",
        "n_episodes = 100\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = QNetwork(input_dim=5, output_dim=n_actions).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# --- Training Loop ---\n",
        "for episode in range(n_episodes):\n",
        "    idx = np.random.randint(len(X_train))\n",
        "    state_np = X_train[idx]\n",
        "    state = torch.FloatTensor(state_np).unsqueeze(0).to(device)\n",
        "    true_cpu = cpu_train[idx]\n",
        "    true_mem = mem_train[idx]\n",
        "\n",
        "    for _ in range(10):\n",
        "        if random.random() < epsilon:\n",
        "            action_idx = random.randint(0, n_actions - 1)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_values = model(state)\n",
        "                action_idx = torch.argmax(q_values).item()\n",
        "\n",
        "        cpu_adj, mem_adj = action_space[action_idx]\n",
        "        predicted_cpu = np.clip(true_cpu + cpu_adj, 0, 1)\n",
        "        predicted_mem = np.clip(true_mem + mem_adj, 0, 1)\n",
        "\n",
        "        # Synthetic latency update\n",
        "        new_latency = state[0][0].item() + 0.05 * cpu_adj - 0.04 * mem_adj\n",
        "        new_latency = np.clip(new_latency, 0, 1)\n",
        "\n",
        "        reward = -abs(new_latency - state[0][0].item()) - 0.01 * (predicted_cpu + predicted_mem)\n",
        "\n",
        "        next_state = torch.FloatTensor([\n",
        "            new_latency, predicted_cpu, predicted_mem, state[0][3].item(), state[0][4].item()\n",
        "        ]).unsqueeze(0).to(device)\n",
        "\n",
        "        memory.append((state, action_idx, reward, next_state))\n",
        "        state = next_state\n",
        "\n",
        "    # Learning\n",
        "    loss_val = None\n",
        "    if len(memory) >= batch_size:\n",
        "        batch = random.sample(memory, batch_size)\n",
        "        states_b, actions_b, rewards_b, next_states_b = zip(*batch)\n",
        "\n",
        "        states_t = torch.cat(states_b).to(device)\n",
        "        actions_t = torch.LongTensor(actions_b).unsqueeze(1).to(device)\n",
        "        rewards_t = torch.FloatTensor(rewards_b).unsqueeze(1).to(device)\n",
        "        next_states_t = torch.cat(next_states_b).to(device)\n",
        "\n",
        "        q_values = model(states_t).gather(1, actions_t)\n",
        "        with torch.no_grad():\n",
        "            max_next_q = model(next_states_t).max(1)[0].unsqueeze(1)\n",
        "            target = rewards_t + gamma * max_next_q\n",
        "\n",
        "        loss = loss_fn(q_values, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_val = loss.item()\n",
        "\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    if episode % 10 == 0 and loss_val is not None:\n",
        "        print(f\"Episode {episode}, Loss: {loss_val:.4f}, Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "# --- Evaluation on Test Set (Accuracy as avg reward) ---\n",
        "total_reward = 0\n",
        "n_test_episodes = 20\n",
        "\n",
        "for _ in range(n_test_episodes):\n",
        "    idx = np.random.randint(len(X_test))\n",
        "    state_np = X_test[idx]\n",
        "    state = torch.FloatTensor(state_np).unsqueeze(0).to(device)\n",
        "    true_cpu = cpu_test[idx]\n",
        "    true_mem = mem_test[idx]\n",
        "\n",
        "    episode_reward = 0\n",
        "    for _ in range(5):\n",
        "        with torch.no_grad():\n",
        "            q_values = model(state)\n",
        "            action_idx = torch.argmax(q_values).item()\n",
        "\n",
        "        cpu_adj, mem_adj = action_space[action_idx]\n",
        "        predicted_cpu = np.clip(true_cpu + cpu_adj, 0, 1)\n",
        "        predicted_mem = np.clip(true_mem + mem_adj, 0, 1)\n",
        "\n",
        "        new_latency = state[0][0].item() + 0.05 * cpu_adj - 0.04 * mem_adj\n",
        "        new_latency = np.clip(new_latency, 0, 1)\n",
        "\n",
        "        reward = -abs(new_latency - state[0][0].item()) - 0.01 * (predicted_cpu + predicted_mem)\n",
        "        episode_reward += reward\n",
        "\n",
        "        state = torch.FloatTensor([\n",
        "            new_latency, predicted_cpu, predicted_mem, state[0][3].item(), state[0][4].item()\n",
        "        ]).unsqueeze(0).to(device)\n",
        "\n",
        "    total_reward += episode_reward\n",
        "\n",
        "avg_reward = total_reward / n_test_episodes\n",
        "print(f\"\\nAverage Test Reward: {avg_reward:.4f} (Higher is better)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Service 2"
      ],
      "metadata": {
        "id": "3bbRzPXFyf8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import deque\n",
        "\n",
        "# Load and preprocess data\n",
        "df = pd.read_csv(\"Service2.csv\")\n",
        "\n",
        "features = ['latency_ms', 'cpu_usage_pct', 'memory_usage_pct', 'cpu_allocated', 'memory_allocated']\n",
        "scaler = MinMaxScaler()\n",
        "df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "X = df[features].values\n",
        "cpu_usage = df['cpu_usage_pct'].values\n",
        "mem_usage = df['memory_usage_pct'].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, cpu_train, cpu_test, mem_train, mem_test = train_test_split(\n",
        "    X, cpu_usage, mem_usage, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Discrete action space\n",
        "action_space = [\n",
        "    (-1, -1), (-1, 0), (-1, 1),\n",
        "    (0, -1),  (0, 0),  (0, 1),\n",
        "    (1, -1),  (1, 0),  (1, 1)\n",
        "]\n",
        "n_actions = len(action_space)\n",
        "\n",
        "# Q-Network definition\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Initialize everything\n",
        "memory = deque(maxlen=10000)\n",
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_decay = 0.995\n",
        "epsilon_min = 0.1\n",
        "batch_size = 64\n",
        "lr = 1e-3\n",
        "n_episodes = 100\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = QNetwork(input_dim=5, output_dim=n_actions).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# --- Training Loop ---\n",
        "for episode in range(n_episodes):\n",
        "    idx = np.random.randint(len(X_train))\n",
        "    state_np = X_train[idx]\n",
        "    state = torch.FloatTensor(state_np).unsqueeze(0).to(device)\n",
        "    true_cpu = cpu_train[idx]\n",
        "    true_mem = mem_train[idx]\n",
        "\n",
        "    for _ in range(10):\n",
        "        if random.random() < epsilon:\n",
        "            action_idx = random.randint(0, n_actions - 1)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_values = model(state)\n",
        "                action_idx = torch.argmax(q_values).item()\n",
        "\n",
        "        cpu_adj, mem_adj = action_space[action_idx]\n",
        "        predicted_cpu = np.clip(true_cpu + cpu_adj, 0, 1)\n",
        "        predicted_mem = np.clip(true_mem + mem_adj, 0, 1)\n",
        "\n",
        "        # Synthetic latency update\n",
        "        new_latency = state[0][0].item() + 0.05 * cpu_adj - 0.04 * mem_adj\n",
        "        new_latency = np.clip(new_latency, 0, 1)\n",
        "\n",
        "        reward = -abs(new_latency - state[0][0].item()) - 0.01 * (predicted_cpu + predicted_mem)\n",
        "\n",
        "        next_state = torch.FloatTensor([\n",
        "            new_latency, predicted_cpu, predicted_mem, state[0][3].item(), state[0][4].item()\n",
        "        ]).unsqueeze(0).to(device)\n",
        "\n",
        "        memory.append((state, action_idx, reward, next_state))\n",
        "        state = next_state\n",
        "\n",
        "    # Learning\n",
        "    loss_val = None\n",
        "    if len(memory) >= batch_size:\n",
        "        batch = random.sample(memory, batch_size)\n",
        "        states_b, actions_b, rewards_b, next_states_b = zip(*batch)\n",
        "\n",
        "        states_t = torch.cat(states_b).to(device)\n",
        "        actions_t = torch.LongTensor(actions_b).unsqueeze(1).to(device)\n",
        "        rewards_t = torch.FloatTensor(rewards_b).unsqueeze(1).to(device)\n",
        "        next_states_t = torch.cat(next_states_b).to(device)\n",
        "\n",
        "        q_values = model(states_t).gather(1, actions_t)\n",
        "        with torch.no_grad():\n",
        "            max_next_q = model(next_states_t).max(1)[0].unsqueeze(1)\n",
        "            target = rewards_t + gamma * max_next_q\n",
        "\n",
        "        loss = loss_fn(q_values, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_val = loss.item()\n",
        "\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    if episode % 10 == 0 and loss_val is not None:\n",
        "        print(f\"Episode {episode}, Loss: {loss_val:.4f}, Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "# --- Evaluation on Test Set (Accuracy as avg reward) ---\n",
        "total_reward = 0\n",
        "n_test_episodes = 20\n",
        "\n",
        "for _ in range(n_test_episodes):\n",
        "    idx = np.random.randint(len(X_test))\n",
        "    state_np = X_test[idx]\n",
        "    state = torch.FloatTensor(state_np).unsqueeze(0).to(device)\n",
        "    true_cpu = cpu_test[idx]\n",
        "    true_mem = mem_test[idx]\n",
        "\n",
        "    episode_reward = 0\n",
        "    for _ in range(5):\n",
        "        with torch.no_grad():\n",
        "            q_values = model(state)\n",
        "            action_idx = torch.argmax(q_values).item()\n",
        "\n",
        "        cpu_adj, mem_adj = action_space[action_idx]\n",
        "        predicted_cpu = np.clip(true_cpu + cpu_adj, 0, 1)\n",
        "        predicted_mem = np.clip(true_mem + mem_adj, 0, 1)\n",
        "\n",
        "        new_latency = state[0][0].item() + 0.05 * cpu_adj - 0.04 * mem_adj\n",
        "        new_latency = np.clip(new_latency, 0, 1)\n",
        "\n",
        "        reward = -abs(new_latency - state[0][0].item()) - 0.01 * (predicted_cpu + predicted_mem)\n",
        "        episode_reward += reward\n",
        "\n",
        "        state = torch.FloatTensor([\n",
        "            new_latency, predicted_cpu, predicted_mem, state[0][3].item(), state[0][4].item()\n",
        "        ]).unsqueeze(0).to(device)\n",
        "\n",
        "    total_reward += episode_reward\n",
        "\n",
        "avg_reward = total_reward / n_test_episodes\n",
        "print(f\"\\nAverage Test Reward: {avg_reward:.4f} (Higher is better)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EXzNbf4yiHJ",
        "outputId": "d1be12a9-4e3d-46de-b3c3-f81ca8993c52"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10, Loss: 0.0010, Epsilon: 0.95\n",
            "Episode 20, Loss: 0.0020, Epsilon: 0.90\n",
            "Episode 30, Loss: 0.0008, Epsilon: 0.86\n",
            "Episode 40, Loss: 0.0005, Epsilon: 0.81\n",
            "Episode 50, Loss: 0.0005, Epsilon: 0.77\n",
            "Episode 60, Loss: 0.0004, Epsilon: 0.74\n",
            "Episode 70, Loss: 0.0005, Epsilon: 0.70\n",
            "Episode 80, Loss: 0.0004, Epsilon: 0.67\n",
            "Episode 90, Loss: 0.0007, Epsilon: 0.63\n",
            "\n",
            "Average Test Reward: -0.1333 (Higher is better)\n"
          ]
        }
      ]
    }
  ]
}