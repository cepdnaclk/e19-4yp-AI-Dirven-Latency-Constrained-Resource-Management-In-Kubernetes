{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2657904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "import logging\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08aa27c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Timestamp', 'Service', 'CPU Request', 'Memory Request', 'CPU Limit',\n",
      "       'Memory Limit', 'Latency', 'CPU Usage', 'Memory Usage'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Paths to service 1 datasets\n",
    "cpu_path_s1 = \"../../results/prometheus_data/service1_cpu_limit_reduction.csv\"\n",
    "memory_path_s1 = \"../../results/prometheus_data/new datasets/service1_memory_limit_reduction.csv\"\n",
    "both_path_s1 = \"../../results/prometheus_data/service1_both_limits_reduction.csv\"\n",
    "\n",
    "# Import datasets\n",
    "df_cpu_s1 = pd.read_csv(cpu_path_s1)\n",
    "df_memory_s1 = pd.read_csv(memory_path_s1)\n",
    "df_both_s1 = pd.read_csv(both_path_s1)\n",
    "\n",
    "df_all_s1 = pd.concat([df_cpu_s1, df_memory_s1, df_both_s1], ignore_index=True)\n",
    "print(df_all_s1.columns)\n",
    "\n",
    "# Paths to service 2 datasets\n",
    "cpu_path_s2 = \"../../results/prometheus_data/service2_cpu_limit_reduction.csv\"\n",
    "memory_path_s2 = \"../../results/prometheus_data/service2_memory_limit_reduction.csv\"\n",
    "both_path_s2 = \"../../results/prometheus_data/service2_both_limit_reduction.csv\"\n",
    "\n",
    "# Import datasets\n",
    "df_cpu_s2 = pd.read_csv(cpu_path_s2)\n",
    "df_memory_s2 = pd.read_csv(memory_path_s2)\n",
    "df_both_s2 = pd.read_csv(both_path_s2)\n",
    "\n",
    "# Combine all three DataFrames\n",
    "df_all_s2 = pd.concat([df_cpu_s2, df_memory_s2, df_both_s2], ignore_index=True)\n",
    "\n",
    "# Paths to datasets\n",
    "cpu_path_hg = \"../../results/prometheus_data/hashgen_cpu_limit_reduction.csv\"\n",
    "memory_path_hg = \"../../results/prometheus_data/hashgen_memory_limit_reduction.csv\"\n",
    "both_path_hg = \"../../results/prometheus_data/hashgen_both_limit_reduction.csv\"\n",
    "\n",
    "# Import datasets\n",
    "df_cpu_hg = pd.read_csv(cpu_path_hg)\n",
    "df_memory_hg = pd.read_csv(memory_path_hg)\n",
    "df_both_hg = pd.read_csv(both_path_hg)\n",
    "\n",
    "# Combine all three DataFrames\n",
    "df_all_hg = pd.concat([df_cpu_hg, df_memory_hg, df_both_hg], ignore_index=True)\n",
    "\n",
    "# Paths to datasets\n",
    "cpu_path_rp = \"../../results/prometheus_data/ranspw_cpu_limit_reduction.csv\"\n",
    "memory_path_rp = \"../../results/prometheus_data/randpw_memory_limit_reduction.csv\"\n",
    "both_path_rp = \"../../results/prometheus_data/randpw_both_limits_reduction.csv\"\n",
    "\n",
    "# Import datasets\n",
    "df_cpu_rp = pd.read_csv(cpu_path_rp)\n",
    "df_memory_rp = pd.read_csv(memory_path_rp)\n",
    "df_both_rp = pd.read_csv(both_path_rp)\n",
    "\n",
    "# Combine all three DataFrames\n",
    "df_all_rp = pd.concat([df_cpu_rp, df_memory_rp, df_both_rp], ignore_index=True)\n",
    "\n",
    "configs = {\n",
    "    \"Service 1\": df_all_s1,\n",
    "    \"Service 2\": df_all_s2,\n",
    "    \"HashGen\": df_all_hg,\n",
    "    \"RandPw\": df_all_rp,\n",
    "}\n",
    "\n",
    "test_sizes = [0.3, 0.2, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d3b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model(df, feature_col, test_size=0.2, plot=True):\n",
    "    \n",
    "    df = df.sort_values(\"Timestamp\")\n",
    "    df = df[[feature_col, \"CPU Request\", \"Memory Request\", \"CPU Limit\", \"Memory Limit\", \"Latency\"]].dropna()\n",
    "\n",
    "    features = [\"CPU Request\", \"Memory Request\", \"CPU Limit\", \"Memory Limit\", \"Latency\"]\n",
    "    target = feature_col\n",
    "\n",
    "    # Normalize memory values if needed\n",
    "    if \"Memory\" in feature_col:\n",
    "        df[feature_col] = df[feature_col] / (1024 * 1024)\n",
    "\n",
    "    # Scale features\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    X_scaled = feature_scaler.fit_transform(df[features])\n",
    "\n",
    "    # Scale target\n",
    "    target_scaler = MinMaxScaler()\n",
    "    y_scaled = target_scaler.fit_transform(df[[target]]).ravel()\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=test_size, shuffle=False)\n",
    "\n",
    "    # Model training\n",
    "    model = XGBRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "\n",
    "    # Inverse scaling\n",
    "    pred_train_inv = target_scaler.inverse_transform(pred_train.reshape(-1, 1)).ravel()\n",
    "    y_train_inv = target_scaler.inverse_transform(y_train.reshape(-1, 1)).ravel()\n",
    "    pred_test_inv = target_scaler.inverse_transform(pred_test.reshape(-1, 1)).ravel()\n",
    "    y_test_inv = target_scaler.inverse_transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "    # Evaluation\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_inv, pred_train_inv))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_inv, pred_test_inv))\n",
    "    train_r2 = r2_score(y_train_inv, pred_train_inv)\n",
    "    test_r2 = r2_score(y_test_inv, pred_test_inv)\n",
    "\n",
    "    print(f\"{feature_col} - Train RMSE: {train_rmse:.4f}, R²: {train_r2:.4f}\")\n",
    "    print(f\"{feature_col} - Test  RMSE: {test_rmse:.4f}, R²: {test_r2:.4f}\")\n",
    "\n",
    "    # if plot:\n",
    "    #     plt.figure(figsize=(10, 4))\n",
    "    #     plt.plot(y_test_inv, label=\"Actual\")\n",
    "    #     plt.plot(pred_test_inv, label=\"Predicted\")\n",
    "    #     plt.title(f\"{feature_col} Prediction (XGBoost)\")\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e8afb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost for Service 1 - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0065, R²: 0.9505\n",
      "CPU Usage - Test  RMSE: 0.0244, R²: 0.1778\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 5.0943, R²: 0.7758\n",
      "Memory Usage - Test  RMSE: 10.2621, R²: -1.4573\n",
      "\n",
      "Training XGBoost for Service 1 - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0063, R²: 0.9482\n",
      "CPU Usage - Test  RMSE: 0.0263, R²: -1.0619\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 5.2501, R²: 0.7332\n",
      "Memory Usage - Test  RMSE: 9.6428, R²: -0.6867\n",
      "\n",
      "Training XGBoost for Service 1 - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0060, R²: 0.9556\n",
      "CPU Usage - Test  RMSE: 0.0137, R²: -1.3366\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 5.1897, R²: 0.7165\n",
      "Memory Usage - Test  RMSE: 10.2037, R²: -0.1057\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0029, R²: 0.9901\n",
      "CPU Usage - Test  RMSE: 0.0064, R²: 0.9179\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 0.1848, R²: 0.5396\n",
      "Memory Usage - Test  RMSE: 0.3695, R²: -1.2034\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0028, R²: 0.9894\n",
      "CPU Usage - Test  RMSE: 0.0064, R²: 0.8264\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 0.1910, R²: 0.5154\n",
      "Memory Usage - Test  RMSE: 0.3253, R²: -0.7667\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0030, R²: 0.9882\n",
      "CPU Usage - Test  RMSE: 0.0068, R²: 0.2191\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 0.1938, R²: 0.4993\n",
      "Memory Usage - Test  RMSE: 0.2581, R²: -0.4193\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0009, R²: 0.9923\n",
      "CPU Usage - Test  RMSE: 0.0054, R²: 0.8084\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 14.1661, R²: 0.4028\n",
      "Memory Usage - Test  RMSE: 24.2097, R²: -0.0520\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0009, R²: 0.9915\n",
      "CPU Usage - Test  RMSE: 0.0064, R²: 0.7629\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 13.5094, R²: 0.4179\n",
      "Memory Usage - Test  RMSE: 27.0253, R²: -0.0089\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0009, R²: 0.9915\n",
      "CPU Usage - Test  RMSE: 0.0088, R²: 0.3372\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 12.7637, R²: 0.4245\n",
      "Memory Usage - Test  RMSE: 36.8297, R²: -0.1488\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0009, R²: 0.7789\n",
      "CPU Usage - Test  RMSE: 0.0036, R²: -6.3065\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 1.0845, R²: 0.9730\n",
      "Memory Usage - Test  RMSE: 12.8084, R²: -1.3331\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0009, R²: 0.7690\n",
      "CPU Usage - Test  RMSE: 0.0049, R²: -10.9468\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 1.0579, R²: 0.9727\n",
      "Memory Usage - Test  RMSE: 14.8203, R²: -1.7036\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0009, R²: 0.7498\n",
      "CPU Usage - Test  RMSE: 0.0040, R²: -10.0899\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 1.2953, R²: 0.9696\n",
      "Memory Usage - Test  RMSE: 5.0125, R²: -1.6796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, df in configs.items():\n",
    "    for test_size in test_sizes:\n",
    "        print(f\"Training XGBoost for {name} - CPU Usage with test size {test_size}\")\n",
    "        model_cpu = train_xgboost_model(df, \"CPU Usage\", test_size)\n",
    "\n",
    "        print(f\"Training XGBoost for {name} - Memory Usage with test size {test_size}\")\n",
    "        model_mem = train_xgboost_model(df, \"Memory Usage\", test_size)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4229fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model_optuna(df, feature_col, test_size=0.2, plot=True):\n",
    "    df = df.sort_values(\"Timestamp\")\n",
    "    df = df[[feature_col, \"CPU Request\", \"Memory Request\", \"CPU Limit\", \"Memory Limit\", \"Latency\"]].dropna()\n",
    "\n",
    "    features = [\"CPU Request\", \"Memory Request\", \"CPU Limit\", \"Memory Limit\", \"Latency\"]\n",
    "    target = feature_col\n",
    "\n",
    "    if \"Memory\" in feature_col:\n",
    "        df[feature_col] = df[feature_col] / (1024 * 1024)\n",
    "\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    X_scaled = feature_scaler.fit_transform(df[features])\n",
    "\n",
    "    target_scaler = MinMaxScaler()\n",
    "    y_scaled = target_scaler.fit_transform(df[[target]]).ravel()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=test_size, shuffle=False)\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 300),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 5),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 5),\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(X_train, y_train, verbose=False)\n",
    "        preds = model.predict(X_test)\n",
    "        rmse = mean_squared_error(y_test, preds)\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=50, show_progress_bar=False)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_params[\"random_state\"] = 42\n",
    "    model = XGBRegressor(**best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "\n",
    "    pred_train_inv = target_scaler.inverse_transform(pred_train.reshape(-1, 1)).ravel()\n",
    "    y_train_inv = target_scaler.inverse_transform(y_train.reshape(-1, 1)).ravel()\n",
    "    pred_test_inv = target_scaler.inverse_transform(pred_test.reshape(-1, 1)).ravel()\n",
    "    y_test_inv = target_scaler.inverse_transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_inv, pred_train_inv))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_inv, pred_test_inv))\n",
    "    train_r2 = r2_score(y_train_inv, pred_train_inv)\n",
    "    test_r2 = r2_score(y_test_inv, pred_test_inv)\n",
    "\n",
    "    print(f\"{feature_col} - Train RMSE: {train_rmse:.4f}, R²: {train_r2:.4f}\")\n",
    "    print(f\"{feature_col} - Test  RMSE: {test_rmse:.4f}, R²: {test_r2:.4f}\")\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "    # if plot:\n",
    "    #     plt.figure(figsize=(10, 4))\n",
    "    #     plt.plot(y_test_inv, label=\"Actual\")\n",
    "    #     plt.plot(pred_test_inv, label=\"Predicted\")\n",
    "    #     plt.title(f\"{feature_col} Prediction (XGBoost + Optuna)\")\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff9ac636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost for Service 1 - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0096, R²: 0.8903\n",
      "CPU Usage - Test  RMSE: 0.0155, R²: 0.6663\n",
      "Best Hyperparameters: {'n_estimators': 256, 'max_depth': 6, 'learning_rate': 0.28350844232325434, 'subsample': 0.7453086894307293, 'colsample_bytree': 0.6238433100192166, 'gamma': 0.2005216134452803, 'reg_alpha': 2.432652056090008, 'reg_lambda': 3.5465864877372892, 'random_state': 42}\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 9.5373, R²: 0.2142\n",
      "Memory Usage - Test  RMSE: 6.6657, R²: -0.0367\n",
      "Best Hyperparameters: {'n_estimators': 240, 'max_depth': 4, 'learning_rate': 0.09509437855002156, 'subsample': 0.804322076652626, 'colsample_bytree': 0.9546317994627257, 'gamma': 0.2845480990479783, 'reg_alpha': 0.49434043421154605, 'reg_lambda': 3.8233420369325732, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 1 - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0082, R²: 0.9120\n",
      "CPU Usage - Test  RMSE: 0.0153, R²: 0.3046\n",
      "Best Hyperparameters: {'n_estimators': 107, 'max_depth': 3, 'learning_rate': 0.148724920602601, 'subsample': 0.9999302152562191, 'colsample_bytree': 0.7218005207036977, 'gamma': 0.03215208800463998, 'reg_alpha': 0.06752131750725132, 'reg_lambda': 0.22646092505855453, 'random_state': 42}\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 7.4114, R²: 0.4682\n",
      "Memory Usage - Test  RMSE: 8.9109, R²: -0.4404\n",
      "Best Hyperparameters: {'n_estimators': 121, 'max_depth': 7, 'learning_rate': 0.07084314813415918, 'subsample': 0.7055868189690354, 'colsample_bytree': 0.7244140316179842, 'gamma': 0.013517074311710356, 'reg_alpha': 0.9225358150192735, 'reg_lambda': 0.7489065441601634, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 1 - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0066, R²: 0.9472\n",
      "CPU Usage - Test  RMSE: 0.0129, R²: -1.0739\n",
      "Best Hyperparameters: {'n_estimators': 289, 'max_depth': 4, 'learning_rate': 0.14325442300080377, 'subsample': 0.60165022306163, 'colsample_bytree': 0.9287026842829298, 'gamma': 0.00175434355561498, 'reg_alpha': 0.000691947147605898, 'reg_lambda': 2.2025246312835187, 'random_state': 42}\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 7.7963, R²: 0.3601\n",
      "Memory Usage - Test  RMSE: 11.3695, R²: -0.3728\n",
      "Best Hyperparameters: {'n_estimators': 102, 'max_depth': 5, 'learning_rate': 0.201682665471478, 'subsample': 0.9294537590692671, 'colsample_bytree': 0.6023969847680567, 'gamma': 0.014545423560501734, 'reg_alpha': 4.844183483569305, 'reg_lambda': 4.655050307705415, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0036, R²: 0.9850\n",
      "CPU Usage - Test  RMSE: 0.0065, R²: 0.9159\n",
      "Best Hyperparameters: {'n_estimators': 295, 'max_depth': 12, 'learning_rate': 0.29816949057519226, 'subsample': 0.6923000919961693, 'colsample_bytree': 0.7147738877216663, 'gamma': 0.002242764616891227, 'reg_alpha': 0.997209099791085, 'reg_lambda': 1.039785686386769, 'random_state': 42}\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 0.2724, R²: -0.0001\n",
      "Memory Usage - Test  RMSE: 0.2638, R²: -0.1233\n",
      "Best Hyperparameters: {'n_estimators': 185, 'max_depth': 6, 'learning_rate': 0.23323680447695525, 'subsample': 0.6557558707046228, 'colsample_bytree': 0.9817608427478867, 'gamma': 2.914803461141776, 'reg_alpha': 3.5215000151992957, 'reg_lambda': 4.532193694858731, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0045, R²: 0.9740\n",
      "CPU Usage - Test  RMSE: 0.0059, R²: 0.8511\n",
      "Best Hyperparameters: {'n_estimators': 194, 'max_depth': 11, 'learning_rate': 0.1858319092041199, 'subsample': 0.7649986823899066, 'colsample_bytree': 0.6332247004481312, 'gamma': 0.24704059830049985, 'reg_alpha': 0.27077476736074063, 'reg_lambda': 3.8939872128488506, 'random_state': 42}\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 0.2743, R²: -0.0000\n",
      "Memory Usage - Test  RMSE: 0.2454, R²: -0.0049\n",
      "Best Hyperparameters: {'n_estimators': 226, 'max_depth': 6, 'learning_rate': 0.20568099641877435, 'subsample': 0.6878805040610567, 'colsample_bytree': 0.9747482953190836, 'gamma': 3.784913272745097, 'reg_alpha': 4.480043541998375, 'reg_lambda': 2.4775994987184866, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0045, R²: 0.9728\n",
      "CPU Usage - Test  RMSE: 0.0049, R²: 0.5832\n",
      "Best Hyperparameters: {'n_estimators': 102, 'max_depth': 11, 'learning_rate': 0.28246371753838706, 'subsample': 0.7818646427392762, 'colsample_bytree': 0.7311106076049844, 'gamma': 0.16991197774988434, 'reg_alpha': 1.2468754156388908, 'reg_lambda': 1.8493285945053897, 'random_state': 42}\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 0.2740, R²: -0.0000\n",
      "Memory Usage - Test  RMSE: 0.2167, R²: -0.0002\n",
      "Best Hyperparameters: {'n_estimators': 107, 'max_depth': 4, 'learning_rate': 0.29077544483769097, 'subsample': 0.6168017059631311, 'colsample_bytree': 0.9443239378655616, 'gamma': 4.991707065687263, 'reg_alpha': 0.0029409844807549157, 'reg_lambda': 0.033456847033310505, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0014, R²: 0.9807\n",
      "CPU Usage - Test  RMSE: 0.0037, R²: 0.9104\n",
      "Best Hyperparameters: {'n_estimators': 130, 'max_depth': 4, 'learning_rate': 0.03346810618577832, 'subsample': 0.945851293304947, 'colsample_bytree': 0.8014619266456458, 'gamma': 0.07920318548994451, 'reg_alpha': 3.2168171949580784, 'reg_lambda': 1.1856417642906516, 'random_state': 42}\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 15.8133, R²: 0.2558\n",
      "Memory Usage - Test  RMSE: 20.6949, R²: 0.2313\n",
      "Best Hyperparameters: {'n_estimators': 207, 'max_depth': 8, 'learning_rate': 0.11046499350451255, 'subsample': 0.6334243854947218, 'colsample_bytree': 0.6315792252416236, 'gamma': 0.3707212026242449, 'reg_alpha': 2.782872726938439, 'reg_lambda': 3.926166960677409, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0017, R²: 0.9711\n",
      "CPU Usage - Test  RMSE: 0.0044, R²: 0.8864\n",
      "Best Hyperparameters: {'n_estimators': 155, 'max_depth': 9, 'learning_rate': 0.030647793971887948, 'subsample': 0.7927431618691472, 'colsample_bytree': 0.9983964572756205, 'gamma': 0.5399842337388998, 'reg_alpha': 4.190194797540647, 'reg_lambda': 2.561336564967016, 'random_state': 42}\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 15.4302, R²: 0.2406\n",
      "Memory Usage - Test  RMSE: 24.2338, R²: 0.1887\n",
      "Best Hyperparameters: {'n_estimators': 150, 'max_depth': 9, 'learning_rate': 0.22784390643976388, 'subsample': 0.8214460067986052, 'colsample_bytree': 0.748812494881732, 'gamma': 0.6829004184612021, 'reg_alpha': 2.5333652792102748, 'reg_lambda': 3.7766423646990948, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0019, R²: 0.9588\n",
      "CPU Usage - Test  RMSE: 0.0059, R²: 0.6954\n",
      "Best Hyperparameters: {'n_estimators': 209, 'max_depth': 8, 'learning_rate': 0.010785834494348511, 'subsample': 0.7635949910061135, 'colsample_bytree': 0.9248370264350001, 'gamma': 0.15922884886003696, 'reg_alpha': 1.8321482898660642, 'reg_lambda': 4.519775141000901, 'random_state': 42}\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 14.5417, R²: 0.2530\n",
      "Memory Usage - Test  RMSE: 33.7534, R²: 0.0351\n",
      "Best Hyperparameters: {'n_estimators': 233, 'max_depth': 8, 'learning_rate': 0.28807199902085506, 'subsample': 0.8562638270235565, 'colsample_bytree': 0.9845178398124551, 'gamma': 0.3028214253890895, 'reg_alpha': 3.68868215928572, 'reg_lambda': 4.4676944542576695, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0014, R²: 0.3865\n",
      "CPU Usage - Test  RMSE: 0.0012, R²: 0.1703\n",
      "Best Hyperparameters: {'n_estimators': 233, 'max_depth': 5, 'learning_rate': 0.24321223158012464, 'subsample': 0.6168939487862473, 'colsample_bytree': 0.6105037621568979, 'gamma': 1.891978079958868, 'reg_alpha': 4.0089497937239695, 'reg_lambda': 0.01187579862126549, 'random_state': 42}\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 4.9642, R²: 0.4338\n",
      "Memory Usage - Test  RMSE: 8.6174, R²: -0.0561\n",
      "Best Hyperparameters: {'n_estimators': 194, 'max_depth': 6, 'learning_rate': 0.12183845569185897, 'subsample': 0.6033179917477087, 'colsample_bytree': 0.9855772768430918, 'gamma': 4.80208558396041, 'reg_alpha': 4.734750197136217, 'reg_lambda': 4.948002197531323, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0014, R²: 0.4072\n",
      "CPU Usage - Test  RMSE: 0.0013, R²: 0.1624\n",
      "Best Hyperparameters: {'n_estimators': 282, 'max_depth': 10, 'learning_rate': 0.21816086412585073, 'subsample': 0.9447672916873027, 'colsample_bytree': 0.6370169972178816, 'gamma': 1.4919739317402891, 'reg_alpha': 2.281646747413837, 'reg_lambda': 2.0363301554429385, 'random_state': 42}\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 4.5366, R²: 0.4976\n",
      "Memory Usage - Test  RMSE: 10.3944, R²: -0.3299\n",
      "Best Hyperparameters: {'n_estimators': 163, 'max_depth': 8, 'learning_rate': 0.08446313505253253, 'subsample': 0.6285705036987503, 'colsample_bytree': 0.8246864045534587, 'gamma': 4.6840321750546225, 'reg_alpha': 3.3041594725763375, 'reg_lambda': 0.7052313855348232, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0014, R²: 0.3558\n",
      "CPU Usage - Test  RMSE: 0.0012, R²: -0.0393\n",
      "Best Hyperparameters: {'n_estimators': 177, 'max_depth': 6, 'learning_rate': 0.29939590440398667, 'subsample': 0.9493279706920922, 'colsample_bytree': 0.6900079696506844, 'gamma': 3.1424463171830923, 'reg_alpha': 3.886734549728798, 'reg_lambda': 2.817879803753685, 'random_state': 42}\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 2.6104, R²: 0.8764\n",
      "Memory Usage - Test  RMSE: 3.1184, R²: -0.0371\n",
      "Best Hyperparameters: {'n_estimators': 186, 'max_depth': 3, 'learning_rate': 0.23668756726839366, 'subsample': 0.6012359130610446, 'colsample_bytree': 0.6815169087582023, 'gamma': 0.00717443455387784, 'reg_alpha': 3.7211472146441764, 'reg_lambda': 4.114245974453361, 'random_state': 42}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, df in configs.items():\n",
    "    for test_size in test_sizes:\n",
    "        print(f\"Training XGBoost for {name} - CPU Usage with test size {test_size}\")\n",
    "        model_cpu = train_xgboost_model_optuna(df, \"CPU Usage\", test_size)\n",
    "\n",
    "        print(f\"Training XGBoost for {name} - Memory Usage with test size {test_size}\")\n",
    "        model_mem = train_xgboost_model_optuna(df, \"Memory Usage\", test_size)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b10516cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_features(df, window=3):\n",
    "    df = df.copy()\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='mixed')\n",
    "\n",
    "    df = df.sort_values(['Service', 'Timestamp'])  # Service-wise time sorting\n",
    "    df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "    # Rolling averages per service\n",
    "    for col in ['CPU Usage', 'Memory Usage', 'Latency']:\n",
    "        df[f'{col}_RollingMean'] = df.groupby('Service')[col].transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "        df[f'{col}_RollingSTD'] = df.groupby('Service')[col].transform(lambda x: x.rolling(window, min_periods=1).std())\n",
    "\n",
    "    # Spike detection\n",
    "    df[\"CPU_Spike\"] = df[\"CPU Usage\"] - df[\"CPU Usage_RollingMean\"]\n",
    "    df[\"Memory_Spike\"] = df[\"Memory Usage\"] - df[\"Memory Usage_RollingMean\"]\n",
    "\n",
    "    # Latency trend direction\n",
    "    df[\"Latency_Trend\"] = df.groupby(\"Service\")[\"Latency\"].transform(lambda x: \n",
    "                    x.diff().fillna(0).apply(lambda y: 1 if y > 0 else (-1 if y < 0 else 0)))\n",
    "\n",
    "    df.reset_index(inplace=True)  # Reset index to include Timestamp again\n",
    "    df.dropna(inplace=True)  # Optional: drop rows with NaNs from rolling\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee103217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model_new_features(df, feature_col, test_size=0.2, plot=True):\n",
    "    df = add_rolling_features(df)\n",
    "    df = df.sort_values(\"Timestamp\")\n",
    "    df = df[[feature_col, \"CPU Request\", \"Memory Request\", \"CPU Limit\", \"Memory Limit\", \"Latency\",\n",
    "    \"CPU Usage_RollingMean\", \"Memory Usage_RollingMean\", \"Latency_RollingMean\",\n",
    "    \"CPU Usage_RollingSTD\", \"Memory Usage_RollingSTD\", \"Latency_RollingSTD\",\n",
    "    \"CPU_Spike\", \"Memory_Spike\", \"Latency_Trend\"]].dropna()\n",
    "\n",
    "    features = [\n",
    "    \"CPU Request\", \"Memory Request\", \"CPU Limit\", \"Memory Limit\", \"Latency\",\n",
    "    \"CPU Usage_RollingMean\", \"Memory Usage_RollingMean\", \"Latency_RollingMean\",\n",
    "    \"CPU Usage_RollingSTD\", \"Memory Usage_RollingSTD\", \"Latency_RollingSTD\",\n",
    "    \"CPU_Spike\", \"Memory_Spike\", \"Latency_Trend\"\n",
    "    ]\n",
    "\n",
    "    target = feature_col\n",
    "\n",
    "    if \"Memory\" in feature_col:\n",
    "        df[feature_col] = df[feature_col] / (1024 * 1024)\n",
    "\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    X_scaled = feature_scaler.fit_transform(df[features])\n",
    "\n",
    "    target_scaler = MinMaxScaler()\n",
    "    y_scaled = target_scaler.fit_transform(df[[target]]).ravel()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=test_size, shuffle=False)\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 300),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 5),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 5),\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(X_train, y_train, verbose=False)\n",
    "        preds = model.predict(X_test)\n",
    "        rmse = mean_squared_error(y_test, preds)\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=50, show_progress_bar=False)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_params[\"random_state\"] = 42\n",
    "    model = XGBRegressor(**best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "\n",
    "    pred_train_inv = target_scaler.inverse_transform(pred_train.reshape(-1, 1)).ravel()\n",
    "    y_train_inv = target_scaler.inverse_transform(y_train.reshape(-1, 1)).ravel()\n",
    "    pred_test_inv = target_scaler.inverse_transform(pred_test.reshape(-1, 1)).ravel()\n",
    "    y_test_inv = target_scaler.inverse_transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_inv, pred_train_inv))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_inv, pred_test_inv))\n",
    "    train_r2 = r2_score(y_train_inv, pred_train_inv)\n",
    "    test_r2 = r2_score(y_test_inv, pred_test_inv)\n",
    "\n",
    "    print(f\"{feature_col} - Train RMSE: {train_rmse:.4f}, R²: {train_r2:.4f}\")\n",
    "    print(f\"{feature_col} - Test  RMSE: {test_rmse:.4f}, R²: {test_r2:.4f}\")\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "    # if plot:\n",
    "    #     plt.figure(figsize=(10, 4))\n",
    "    #     plt.plot(y_test_inv, label=\"Actual\")\n",
    "    #     plt.plot(pred_test_inv, label=\"Predicted\")\n",
    "    #     plt.title(f\"{feature_col} Prediction (XGBoost + Optuna)\")\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dd76b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost for Service 1 - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0015, R²: 0.9974\n",
      "CPU Usage - Test  RMSE: 0.0016, R²: 0.9963\n",
      "Best Hyperparameters: {'n_estimators': 216, 'max_depth': 12, 'learning_rate': 0.25352981072649533, 'subsample': 0.7310223492156127, 'colsample_bytree': 0.9941254364331276, 'gamma': 0.05681613905306729, 'reg_alpha': 0.05148254568313848, 'reg_lambda': 2.135120151050652, 'random_state': 42}\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 7.6432, R²: 0.4950\n",
      "Memory Usage - Test  RMSE: 5.6337, R²: 0.2594\n",
      "Best Hyperparameters: {'n_estimators': 220, 'max_depth': 7, 'learning_rate': 0.046549040797827, 'subsample': 0.9560199730091368, 'colsample_bytree': 0.9391408415612624, 'gamma': 0.4680883399734426, 'reg_alpha': 4.624819103116447, 'reg_lambda': 2.81189314725288, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 1 - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0012, R²: 0.9981\n",
      "CPU Usage - Test  RMSE: 0.0020, R²: 0.9880\n",
      "Best Hyperparameters: {'n_estimators': 112, 'max_depth': 8, 'learning_rate': 0.29879243617877427, 'subsample': 0.8810318999567386, 'colsample_bytree': 0.8793461909128694, 'gamma': 0.02820276076673716, 'reg_alpha': 0.03543961010243979, 'reg_lambda': 2.0816389854610793, 'random_state': 42}\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 2.2845, R²: 0.9494\n",
      "Memory Usage - Test  RMSE: 6.2511, R²: 0.2914\n",
      "Best Hyperparameters: {'n_estimators': 198, 'max_depth': 6, 'learning_rate': 0.13084954016662428, 'subsample': 0.9146278342849864, 'colsample_bytree': 0.9868822248239354, 'gamma': 0.11299358690326552, 'reg_alpha': 0.2134454080587601, 'reg_lambda': 1.1475737199958476, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 1 - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0032, R²: 0.9878\n",
      "CPU Usage - Test  RMSE: 0.0027, R²: 0.9079\n",
      "Best Hyperparameters: {'n_estimators': 130, 'max_depth': 6, 'learning_rate': 0.12766705340210002, 'subsample': 0.6988145438373454, 'colsample_bytree': 0.8079806483130143, 'gamma': 0.05606191957094264, 'reg_alpha': 4.395387704304782, 'reg_lambda': 2.3033877357461847, 'random_state': 42}\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 4.8941, R²: 0.7477\n",
      "Memory Usage - Test  RMSE: 8.8905, R²: 0.1606\n",
      "Best Hyperparameters: {'n_estimators': 239, 'max_depth': 8, 'learning_rate': 0.07954152980732283, 'subsample': 0.8965287743609628, 'colsample_bytree': 0.9995951947939872, 'gamma': 0.057357865044275036, 'reg_alpha': 4.617113890249314, 'reg_lambda': 3.3113788908466146, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0007, R²: 0.9994\n",
      "CPU Usage - Test  RMSE: 0.0008, R²: 0.9987\n",
      "Best Hyperparameters: {'n_estimators': 182, 'max_depth': 7, 'learning_rate': 0.1622467694654295, 'subsample': 0.7230358253029406, 'colsample_bytree': 0.9954523958150769, 'gamma': 0.0001703222513484981, 'reg_alpha': 3.456759138843908, 'reg_lambda': 0.8280426923467512, 'random_state': 42}\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 0.0351, R²: 0.9834\n",
      "Memory Usage - Test  RMSE: 0.0462, R²: 0.9655\n",
      "Best Hyperparameters: {'n_estimators': 198, 'max_depth': 12, 'learning_rate': 0.28461164088048135, 'subsample': 0.6716009970542712, 'colsample_bytree': 0.9958368111716888, 'gamma': 0.0140731336096232, 'reg_alpha': 0.9976186464948704, 'reg_lambda': 3.8405368180607233, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0007, R²: 0.9993\n",
      "CPU Usage - Test  RMSE: 0.0009, R²: 0.9968\n",
      "Best Hyperparameters: {'n_estimators': 232, 'max_depth': 7, 'learning_rate': 0.08701639818634528, 'subsample': 0.9117130093828574, 'colsample_bytree': 0.8997731734703099, 'gamma': 0.005210050936762037, 'reg_alpha': 2.7439305086141186, 'reg_lambda': 0.0976688516838613, 'random_state': 42}\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 0.0426, R²: 0.9759\n",
      "Memory Usage - Test  RMSE: 0.0529, R²: 0.9534\n",
      "Best Hyperparameters: {'n_estimators': 254, 'max_depth': 12, 'learning_rate': 0.23606649618350836, 'subsample': 0.8454956897512679, 'colsample_bytree': 0.8572124602070762, 'gamma': 0.026168484452638134, 'reg_alpha': 2.025362800309454, 'reg_lambda': 3.0026323981946237, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0005, R²: 0.9997\n",
      "CPU Usage - Test  RMSE: 0.0009, R²: 0.9877\n",
      "Best Hyperparameters: {'n_estimators': 167, 'max_depth': 11, 'learning_rate': 0.2687818729882642, 'subsample': 0.6379568846983381, 'colsample_bytree': 0.8957730997398329, 'gamma': 0.003780854272653178, 'reg_alpha': 0.5033036270196752, 'reg_lambda': 3.085016545621465, 'random_state': 42}\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 0.0378, R²: 0.9809\n",
      "Memory Usage - Test  RMSE: 0.0469, R²: 0.9531\n",
      "Best Hyperparameters: {'n_estimators': 273, 'max_depth': 6, 'learning_rate': 0.036790072945729696, 'subsample': 0.9159535160603949, 'colsample_bytree': 0.7956342459839131, 'gamma': 0.007630305072953175, 'reg_alpha': 2.4039688678498488, 'reg_lambda': 3.0898785300799694, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0008, R²: 0.9935\n",
      "CPU Usage - Test  RMSE: 0.0014, R²: 0.9864\n",
      "Best Hyperparameters: {'n_estimators': 116, 'max_depth': 4, 'learning_rate': 0.29853112228705203, 'subsample': 0.9531676085544648, 'colsample_bytree': 0.9141840506320882, 'gamma': 0.3294978059661875, 'reg_alpha': 3.7432636653582962, 'reg_lambda': 0.7015113881319174, 'random_state': 42}\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 2.9635, R²: 0.9739\n",
      "Memory Usage - Test  RMSE: 4.1008, R²: 0.9698\n",
      "Best Hyperparameters: {'n_estimators': 290, 'max_depth': 11, 'learning_rate': 0.1771167115920281, 'subsample': 0.9809619405075201, 'colsample_bytree': 0.9960870948685702, 'gamma': 0.1767381058546238, 'reg_alpha': 1.0040184761538502, 'reg_lambda': 1.5437804256774363, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0004, R²: 0.9985\n",
      "CPU Usage - Test  RMSE: 0.0016, R²: 0.9851\n",
      "Best Hyperparameters: {'n_estimators': 288, 'max_depth': 10, 'learning_rate': 0.04419402791178283, 'subsample': 0.99739456217843, 'colsample_bytree': 0.661285301362337, 'gamma': 0.0068882742698437305, 'reg_alpha': 1.0314673907116063, 'reg_lambda': 4.242987395789955, 'random_state': 42}\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 2.9948, R²: 0.9714\n",
      "Memory Usage - Test  RMSE: 4.7388, R²: 0.9690\n",
      "Best Hyperparameters: {'n_estimators': 138, 'max_depth': 5, 'learning_rate': 0.2067211933242407, 'subsample': 0.828061312513522, 'colsample_bytree': 0.9997875782520177, 'gamma': 0.12604817757684472, 'reg_alpha': 1.2156451448452856, 'reg_lambda': 2.7269384120419513, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0006, R²: 0.9961\n",
      "CPU Usage - Test  RMSE: 0.0023, R²: 0.9536\n",
      "Best Hyperparameters: {'n_estimators': 154, 'max_depth': 3, 'learning_rate': 0.13273102991916724, 'subsample': 0.6523455855996281, 'colsample_bytree': 0.7610816394303878, 'gamma': 0.03453408653364773, 'reg_alpha': 3.541684015033285, 'reg_lambda': 2.4403350660415444, 'random_state': 42}\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 1.1824, R²: 0.9951\n",
      "Memory Usage - Test  RMSE: 5.5879, R²: 0.9736\n",
      "Best Hyperparameters: {'n_estimators': 139, 'max_depth': 3, 'learning_rate': 0.12241936122839525, 'subsample': 0.6798997810463417, 'colsample_bytree': 0.9437829292848239, 'gamma': 0.0005687764293899195, 'reg_alpha': 0.3463463504599036, 'reg_lambda': 1.6307160659906281, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0001, R²: 0.9961\n",
      "CPU Usage - Test  RMSE: 0.0002, R²: 0.9799\n",
      "Best Hyperparameters: {'n_estimators': 136, 'max_depth': 10, 'learning_rate': 0.06825123219503373, 'subsample': 0.7707644179112405, 'colsample_bytree': 0.9549218107417056, 'gamma': 0.010349599354457912, 'reg_alpha': 0.8908853547432383, 'reg_lambda': 3.1416924317713337, 'random_state': 42}\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 1.3433, R²: 0.9585\n",
      "Memory Usage - Test  RMSE: 1.2491, R²: 0.9778\n",
      "Best Hyperparameters: {'n_estimators': 132, 'max_depth': 8, 'learning_rate': 0.1499250194575861, 'subsample': 0.6478125456502799, 'colsample_bytree': 0.933162958425033, 'gamma': 0.5291672274684428, 'reg_alpha': 4.00259161932529, 'reg_lambda': 3.7324305521594194, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0002, R²: 0.9865\n",
      "CPU Usage - Test  RMSE: 0.0002, R²: 0.9864\n",
      "Best Hyperparameters: {'n_estimators': 241, 'max_depth': 5, 'learning_rate': 0.15524819193950834, 'subsample': 0.6997252059348694, 'colsample_bytree': 0.9890257622612442, 'gamma': 0.00018620233504407224, 'reg_alpha': 4.925673653059656, 'reg_lambda': 1.3603274930198324, 'random_state': 42}\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 0.9352, R²: 0.9786\n",
      "Memory Usage - Test  RMSE: 1.8111, R²: 0.9596\n",
      "Best Hyperparameters: {'n_estimators': 198, 'max_depth': 8, 'learning_rate': 0.16566648833237924, 'subsample': 0.8510621940515213, 'colsample_bytree': 0.9733945257735508, 'gamma': 0.22848239741047616, 'reg_alpha': 3.8645128078652577, 'reg_lambda': 4.026247415367144, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0001, R²: 0.9947\n",
      "CPU Usage - Test  RMSE: 0.0001, R²: 0.9939\n",
      "Best Hyperparameters: {'n_estimators': 106, 'max_depth': 8, 'learning_rate': 0.2880534733486319, 'subsample': 0.7677905795955552, 'colsample_bytree': 0.9929139988573482, 'gamma': 0.002903905293087705, 'reg_alpha': 1.888652004302469, 'reg_lambda': 4.077899939302454, 'random_state': 42}\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 0.9867, R²: 0.9823\n",
      "Memory Usage - Test  RMSE: 0.3529, R²: 0.9867\n",
      "Best Hyperparameters: {'n_estimators': 207, 'max_depth': 8, 'learning_rate': 0.12343911363245301, 'subsample': 0.7911790792616874, 'colsample_bytree': 0.9635393053755228, 'gamma': 0.3008754494993293, 'reg_alpha': 3.9083667948775873, 'reg_lambda': 2.7895728118570418, 'random_state': 42}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, df in configs.items():\n",
    "    for test_size in test_sizes:\n",
    "        print(f\"Training XGBoost for {name} - CPU Usage with test size {test_size}\")\n",
    "        model_cpu = train_xgboost_model_new_features(df, \"CPU Usage\", test_size)\n",
    "\n",
    "        print(f\"Training XGBoost for {name} - Memory Usage with test size {test_size}\")\n",
    "        model_mem = train_xgboost_model_new_features(df, \"Memory Usage\", test_size)\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
