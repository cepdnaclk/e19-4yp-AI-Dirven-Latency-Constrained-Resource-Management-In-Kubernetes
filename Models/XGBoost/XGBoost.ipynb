{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2657904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "import logging\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08aa27c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Timestamp', 'Service', 'CPU Request', 'Memory Request', 'CPU Limit',\n",
      "       'Memory Limit', 'Latency', 'CPU Usage', 'Memory Usage'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Paths to service 1 datasets\n",
    "cpu_path_s1 = \"../../results/prometheus_data/service1_cpu_limit_reduction.csv\"\n",
    "memory_path_s1 = \"../../results/prometheus_data/new datasets/service1_memory_limit_reduction.csv\"\n",
    "both_path_s1 = \"../../results/prometheus_data/service1_both_limits_reduction.csv\"\n",
    "\n",
    "# Import datasets\n",
    "df_cpu_s1 = pd.read_csv(cpu_path_s1)\n",
    "df_memory_s1 = pd.read_csv(memory_path_s1)\n",
    "df_both_s1 = pd.read_csv(both_path_s1)\n",
    "\n",
    "df_all_s1 = pd.concat([df_cpu_s1, df_memory_s1, df_both_s1], ignore_index=True)\n",
    "print(df_all_s1.columns)\n",
    "\n",
    "# Paths to service 2 datasets\n",
    "cpu_path_s2 = \"../../results/prometheus_data/service2_cpu_limit_reduction.csv\"\n",
    "memory_path_s2 = \"../../results/prometheus_data/service2_memory_limit_reduction.csv\"\n",
    "both_path_s2 = \"../../results/prometheus_data/service2_both_limit_reduction.csv\"\n",
    "\n",
    "# Import datasets\n",
    "df_cpu_s2 = pd.read_csv(cpu_path_s2)\n",
    "df_memory_s2 = pd.read_csv(memory_path_s2)\n",
    "df_both_s2 = pd.read_csv(both_path_s2)\n",
    "\n",
    "# Combine all three DataFrames\n",
    "df_all_s2 = pd.concat([df_cpu_s2, df_memory_s2, df_both_s2], ignore_index=True)\n",
    "\n",
    "# Paths to datasets\n",
    "cpu_path_hg = \"../../results/prometheus_data/hashgen_cpu_limit_reduction.csv\"\n",
    "memory_path_hg = \"../../results/prometheus_data/hashgen_memory_limit_reduction.csv\"\n",
    "both_path_hg = \"../../results/prometheus_data/hashgen_both_limit_reduction.csv\"\n",
    "\n",
    "# Import datasets\n",
    "df_cpu_hg = pd.read_csv(cpu_path_hg)\n",
    "df_memory_hg = pd.read_csv(memory_path_hg)\n",
    "df_both_hg = pd.read_csv(both_path_hg)\n",
    "\n",
    "# Combine all three DataFrames\n",
    "df_all_hg = pd.concat([df_cpu_hg, df_memory_hg, df_both_hg], ignore_index=True)\n",
    "\n",
    "# Paths to datasets\n",
    "cpu_path_rp = \"../../results/prometheus_data/ranspw_cpu_limit_reduction.csv\"\n",
    "memory_path_rp = \"../../results/prometheus_data/randpw_memory_limit_reduction.csv\"\n",
    "both_path_rp = \"../../results/prometheus_data/randpw_both_limits_reduction.csv\"\n",
    "\n",
    "# Import datasets\n",
    "df_cpu_rp = pd.read_csv(cpu_path_rp)\n",
    "df_memory_rp = pd.read_csv(memory_path_rp)\n",
    "df_both_rp = pd.read_csv(both_path_rp)\n",
    "\n",
    "# Combine all three DataFrames\n",
    "df_all_rp = pd.concat([df_cpu_rp, df_memory_rp, df_both_rp], ignore_index=True)\n",
    "\n",
    "configs = {\n",
    "    \"Service 1\": df_all_s1,\n",
    "    \"Service 2\": df_all_s2,\n",
    "    \"HashGen\": df_all_hg,\n",
    "    \"RandPw\": df_all_rp,\n",
    "}\n",
    "\n",
    "test_sizes = [0.3, 0.2, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92d3b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model(df, feature_col, test_size=0.2, plot=True):\n",
    "    \n",
    "    df = df.sort_values(\"Timestamp\")\n",
    "    df = df[[feature_col, \"CPU Request\", \"Memory Request\", \"CPU Limit\", \"Memory Limit\", \"Latency\"]].dropna()\n",
    "\n",
    "    features = [\"CPU Request\", \"Memory Request\", \"CPU Limit\", \"Memory Limit\", \"Latency\"]\n",
    "    target = feature_col\n",
    "\n",
    "    # Normalize memory values if needed\n",
    "    if \"Memory\" in feature_col:\n",
    "        df[feature_col] = df[feature_col] / (1024 * 1024)\n",
    "\n",
    "    # Scale features\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    X_scaled = feature_scaler.fit_transform(df[features])\n",
    "\n",
    "    # Scale target\n",
    "    target_scaler = MinMaxScaler()\n",
    "    y_scaled = target_scaler.fit_transform(df[[target]]).ravel()\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=test_size, shuffle=False)\n",
    "\n",
    "    # Model training\n",
    "    model = XGBRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "\n",
    "    # Inverse scaling\n",
    "    pred_train_inv = target_scaler.inverse_transform(pred_train.reshape(-1, 1)).ravel()\n",
    "    y_train_inv = target_scaler.inverse_transform(y_train.reshape(-1, 1)).ravel()\n",
    "    pred_test_inv = target_scaler.inverse_transform(pred_test.reshape(-1, 1)).ravel()\n",
    "    y_test_inv = target_scaler.inverse_transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "    # Evaluation\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_inv, pred_train_inv))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_inv, pred_test_inv))\n",
    "    train_r2 = r2_score(y_train_inv, pred_train_inv)\n",
    "    test_r2 = r2_score(y_test_inv, pred_test_inv)\n",
    "\n",
    "    print(f\"{feature_col} - Train RMSE: {train_rmse:.4f}, R²: {train_r2:.4f}\")\n",
    "    print(f\"{feature_col} - Test  RMSE: {test_rmse:.4f}, R²: {test_r2:.4f}\")\n",
    "\n",
    "    # if plot:\n",
    "    #     plt.figure(figsize=(10, 4))\n",
    "    #     plt.plot(y_test_inv, label=\"Actual\")\n",
    "    #     plt.plot(pred_test_inv, label=\"Predicted\")\n",
    "    #     plt.title(f\"{feature_col} Prediction (XGBoost)\")\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e8afb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost for Service 1 - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0065, R²: 0.9505\n",
      "CPU Usage - Test  RMSE: 0.0244, R²: 0.1778\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 5.0943, R²: 0.7758\n",
      "Memory Usage - Test  RMSE: 10.2621, R²: -1.4573\n",
      "\n",
      "Training XGBoost for Service 1 - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0063, R²: 0.9482\n",
      "CPU Usage - Test  RMSE: 0.0263, R²: -1.0619\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 5.2501, R²: 0.7332\n",
      "Memory Usage - Test  RMSE: 9.6428, R²: -0.6867\n",
      "\n",
      "Training XGBoost for Service 1 - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0060, R²: 0.9556\n",
      "CPU Usage - Test  RMSE: 0.0137, R²: -1.3366\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 5.1897, R²: 0.7165\n",
      "Memory Usage - Test  RMSE: 10.2037, R²: -0.1057\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0029, R²: 0.9901\n",
      "CPU Usage - Test  RMSE: 0.0064, R²: 0.9179\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 0.1848, R²: 0.5396\n",
      "Memory Usage - Test  RMSE: 0.3695, R²: -1.2034\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0028, R²: 0.9894\n",
      "CPU Usage - Test  RMSE: 0.0064, R²: 0.8264\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 0.1910, R²: 0.5154\n",
      "Memory Usage - Test  RMSE: 0.3253, R²: -0.7667\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0030, R²: 0.9882\n",
      "CPU Usage - Test  RMSE: 0.0068, R²: 0.2191\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 0.1938, R²: 0.4993\n",
      "Memory Usage - Test  RMSE: 0.2581, R²: -0.4193\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0009, R²: 0.9923\n",
      "CPU Usage - Test  RMSE: 0.0054, R²: 0.8084\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 14.1661, R²: 0.4028\n",
      "Memory Usage - Test  RMSE: 24.2097, R²: -0.0520\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0009, R²: 0.9915\n",
      "CPU Usage - Test  RMSE: 0.0064, R²: 0.7629\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 13.5094, R²: 0.4179\n",
      "Memory Usage - Test  RMSE: 27.0253, R²: -0.0089\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0009, R²: 0.9915\n",
      "CPU Usage - Test  RMSE: 0.0088, R²: 0.3372\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 12.7637, R²: 0.4245\n",
      "Memory Usage - Test  RMSE: 36.8297, R²: -0.1488\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0009, R²: 0.7789\n",
      "CPU Usage - Test  RMSE: 0.0036, R²: -6.3065\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 1.0845, R²: 0.9730\n",
      "Memory Usage - Test  RMSE: 12.8084, R²: -1.3331\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0009, R²: 0.7690\n",
      "CPU Usage - Test  RMSE: 0.0049, R²: -10.9468\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 1.0579, R²: 0.9727\n",
      "Memory Usage - Test  RMSE: 14.8203, R²: -1.7036\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0009, R²: 0.7498\n",
      "CPU Usage - Test  RMSE: 0.0040, R²: -10.0899\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 1.2953, R²: 0.9696\n",
      "Memory Usage - Test  RMSE: 5.0125, R²: -1.6796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, df in configs.items():\n",
    "    for test_size in test_sizes:\n",
    "        print(f\"Training XGBoost for {name} - CPU Usage with test size {test_size}\")\n",
    "        model_cpu = train_xgboost_model(df, \"CPU Usage\", test_size)\n",
    "\n",
    "        print(f\"Training XGBoost for {name} - Memory Usage with test size {test_size}\")\n",
    "        model_mem = train_xgboost_model(df, \"Memory Usage\", test_size)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4229fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model_optuna(df, feature_col, test_size=0.2, plot=True):\n",
    "    df = df.sort_values(\"Timestamp\")\n",
    "    df = df[[feature_col, \"CPU Request\", \"Memory Request\", \"CPU Limit\", \"Memory Limit\", \"Latency\"]].dropna()\n",
    "\n",
    "    features = [\"CPU Request\", \"Memory Request\", \"CPU Limit\", \"Memory Limit\", \"Latency\"]\n",
    "    target = feature_col\n",
    "\n",
    "    if \"Memory\" in feature_col:\n",
    "        df[feature_col] = df[feature_col] / (1024 * 1024)\n",
    "\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    X_scaled = feature_scaler.fit_transform(df[features])\n",
    "\n",
    "    target_scaler = MinMaxScaler()\n",
    "    y_scaled = target_scaler.fit_transform(df[[target]]).ravel()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=test_size, shuffle=False)\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 300),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 5),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 5),\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(X_train, y_train, verbose=False)\n",
    "        preds = model.predict(X_test)\n",
    "        rmse = mean_squared_error(y_test, preds)\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=50, show_progress_bar=False)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_params[\"random_state\"] = 42\n",
    "    model = XGBRegressor(**best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "\n",
    "    pred_train_inv = target_scaler.inverse_transform(pred_train.reshape(-1, 1)).ravel()\n",
    "    y_train_inv = target_scaler.inverse_transform(y_train.reshape(-1, 1)).ravel()\n",
    "    pred_test_inv = target_scaler.inverse_transform(pred_test.reshape(-1, 1)).ravel()\n",
    "    y_test_inv = target_scaler.inverse_transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_inv, pred_train_inv))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_inv, pred_test_inv))\n",
    "    train_r2 = r2_score(y_train_inv, pred_train_inv)\n",
    "    test_r2 = r2_score(y_test_inv, pred_test_inv)\n",
    "\n",
    "    print(f\"{feature_col} - Train RMSE: {train_rmse:.4f}, R²: {train_r2:.4f}\")\n",
    "    print(f\"{feature_col} - Test  RMSE: {test_rmse:.4f}, R²: {test_r2:.4f}\")\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "    # if plot:\n",
    "    #     plt.figure(figsize=(10, 4))\n",
    "    #     plt.plot(y_test_inv, label=\"Actual\")\n",
    "    #     plt.plot(pred_test_inv, label=\"Predicted\")\n",
    "    #     plt.title(f\"{feature_col} Prediction (XGBoost + Optuna)\")\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff9ac636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost for Service 1 - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0081, R²: 0.9217\n",
      "CPU Usage - Test  RMSE: 0.0151, R²: 0.6834\n",
      "Best Hyperparameters: {'n_estimators': 230, 'max_depth': 11, 'learning_rate': 0.2099892620425105, 'subsample': 0.8071373947971165, 'colsample_bytree': 0.7038104119110115, 'gamma': 0.001619005658308222, 'reg_alpha': 2.4619132789049236, 'reg_lambda': 0.949408169569826, 'random_state': 42}\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 9.9721, R²: 0.1409\n",
      "Memory Usage - Test  RMSE: 6.7263, R²: -0.0557\n",
      "Best Hyperparameters: {'n_estimators': 164, 'max_depth': 6, 'learning_rate': 0.20351355045572472, 'subsample': 0.8064184220485027, 'colsample_bytree': 0.9123269442125039, 'gamma': 0.537604565100349, 'reg_alpha': 0.3565679717555251, 'reg_lambda': 2.880685799333218, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 1 - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0073, R²: 0.9291\n",
      "CPU Usage - Test  RMSE: 0.0149, R²: 0.3440\n",
      "Best Hyperparameters: {'n_estimators': 198, 'max_depth': 9, 'learning_rate': 0.273396450180896, 'subsample': 0.6911674431817804, 'colsample_bytree': 0.6745375660862466, 'gamma': 0.010651117504128504, 'reg_alpha': 0.15675444510067782, 'reg_lambda': 4.16654724297381, 'random_state': 42}\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 6.8242, R²: 0.5492\n",
      "Memory Usage - Test  RMSE: 8.9978, R²: -0.4686\n",
      "Best Hyperparameters: {'n_estimators': 239, 'max_depth': 11, 'learning_rate': 0.1420043789547971, 'subsample': 0.7373718335418216, 'colsample_bytree': 0.9034965220040824, 'gamma': 0.0025224190768632186, 'reg_alpha': 0.816721179738056, 'reg_lambda': 4.567033567360915, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 1 - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0077, R²: 0.9276\n",
      "CPU Usage - Test  RMSE: 0.0153, R²: -1.9310\n",
      "Best Hyperparameters: {'n_estimators': 162, 'max_depth': 5, 'learning_rate': 0.16678592683075197, 'subsample': 0.7783724427478029, 'colsample_bytree': 0.7301899785537432, 'gamma': 0.003691349722728013, 'reg_alpha': 2.378240014193854, 'reg_lambda': 1.5957097790698274, 'random_state': 42}\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 7.5759, R²: 0.3958\n",
      "Memory Usage - Test  RMSE: 11.3614, R²: -0.3709\n",
      "Best Hyperparameters: {'n_estimators': 224, 'max_depth': 11, 'learning_rate': 0.23696665186564173, 'subsample': 0.7672008038037306, 'colsample_bytree': 0.9481161746711296, 'gamma': 0.0013171815360202913, 'reg_alpha': 4.6040527624736605, 'reg_lambda': 1.702948660113006, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0040, R²: 0.9819\n",
      "CPU Usage - Test  RMSE: 0.0066, R²: 0.9125\n",
      "Best Hyperparameters: {'n_estimators': 274, 'max_depth': 11, 'learning_rate': 0.25856427797414033, 'subsample': 0.7563305498077519, 'colsample_bytree': 0.9451131379161684, 'gamma': 0.0043087375234800995, 'reg_alpha': 2.2351875787880804, 'reg_lambda': 0.5939156186119661, 'random_state': 42}\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 0.2724, R²: -0.0002\n",
      "Memory Usage - Test  RMSE: 0.2636, R²: -0.1214\n",
      "Best Hyperparameters: {'n_estimators': 186, 'max_depth': 7, 'learning_rate': 0.29451536280863444, 'subsample': 0.6167978794497472, 'colsample_bytree': 0.7928060667191625, 'gamma': 4.648348208727725, 'reg_alpha': 3.4229508846192616, 'reg_lambda': 2.057954273901257, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0037, R²: 0.9816\n",
      "CPU Usage - Test  RMSE: 0.0059, R²: 0.8529\n",
      "Best Hyperparameters: {'n_estimators': 159, 'max_depth': 9, 'learning_rate': 0.03272488054714397, 'subsample': 0.7071267354444074, 'colsample_bytree': 0.9355364527561169, 'gamma': 0.018834927323860426, 'reg_alpha': 0.5346496815787094, 'reg_lambda': 4.581331603063355, 'random_state': 42}\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 0.2743, R²: -0.0000\n",
      "Memory Usage - Test  RMSE: 0.2454, R²: -0.0053\n",
      "Best Hyperparameters: {'n_estimators': 299, 'max_depth': 11, 'learning_rate': 0.2409691159511521, 'subsample': 0.6771759796941411, 'colsample_bytree': 0.7253519434688265, 'gamma': 4.436758952986352, 'reg_alpha': 2.1770157216448323, 'reg_lambda': 2.7920205275926624, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0045, R²: 0.9727\n",
      "CPU Usage - Test  RMSE: 0.0054, R²: 0.5091\n",
      "Best Hyperparameters: {'n_estimators': 138, 'max_depth': 5, 'learning_rate': 0.14045700830150473, 'subsample': 0.8462923977638206, 'colsample_bytree': 0.9077494127448419, 'gamma': 0.1464751868380048, 'reg_alpha': 3.6833990467559063, 'reg_lambda': 4.497724873233489, 'random_state': 42}\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 0.2740, R²: -0.0000\n",
      "Memory Usage - Test  RMSE: 0.2167, R²: -0.0001\n",
      "Best Hyperparameters: {'n_estimators': 228, 'max_depth': 8, 'learning_rate': 0.2399312258597311, 'subsample': 0.7363177267124513, 'colsample_bytree': 0.9270818536028361, 'gamma': 4.792471377361513, 'reg_alpha': 3.0411990296332068, 'reg_lambda': 4.971479135229626, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0015, R²: 0.9774\n",
      "CPU Usage - Test  RMSE: 0.0037, R²: 0.9084\n",
      "Best Hyperparameters: {'n_estimators': 238, 'max_depth': 5, 'learning_rate': 0.03031089693239604, 'subsample': 0.9415266681056, 'colsample_bytree': 0.858418824402021, 'gamma': 0.25425284408985416, 'reg_alpha': 2.9746447457077294, 'reg_lambda': 4.091235868210184, 'random_state': 42}\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 16.0460, R²: 0.2337\n",
      "Memory Usage - Test  RMSE: 20.7454, R²: 0.2275\n",
      "Best Hyperparameters: {'n_estimators': 266, 'max_depth': 10, 'learning_rate': 0.11028361834406775, 'subsample': 0.8099294547621222, 'colsample_bytree': 0.7805072610764314, 'gamma': 0.7843427913784554, 'reg_alpha': 3.357762687271376, 'reg_lambda': 1.7275795477271707, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0015, R²: 0.9768\n",
      "CPU Usage - Test  RMSE: 0.0043, R²: 0.8922\n",
      "Best Hyperparameters: {'n_estimators': 140, 'max_depth': 7, 'learning_rate': 0.061753106994644986, 'subsample': 0.7514080135404078, 'colsample_bytree': 0.9239096789728131, 'gamma': 0.1323628931655006, 'reg_alpha': 4.545152206212639, 'reg_lambda': 2.7255576313984804, 'random_state': 42}\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 15.4768, R²: 0.2360\n",
      "Memory Usage - Test  RMSE: 24.2966, R²: 0.1845\n",
      "Best Hyperparameters: {'n_estimators': 273, 'max_depth': 6, 'learning_rate': 0.19974169512749484, 'subsample': 0.6415246895913861, 'colsample_bytree': 0.632723192277918, 'gamma': 0.7500772729390601, 'reg_alpha': 1.6796798471846586, 'reg_lambda': 4.139671187488289, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0015, R²: 0.9739\n",
      "CPU Usage - Test  RMSE: 0.0060, R²: 0.6927\n",
      "Best Hyperparameters: {'n_estimators': 288, 'max_depth': 7, 'learning_rate': 0.010811055465312386, 'subsample': 0.6964077423519914, 'colsample_bytree': 0.9558887565509727, 'gamma': 0.055986326272383105, 'reg_alpha': 2.495974491449982, 'reg_lambda': 1.5328019198716336, 'random_state': 42}\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 14.6269, R²: 0.2442\n",
      "Memory Usage - Test  RMSE: 33.6597, R²: 0.0405\n",
      "Best Hyperparameters: {'n_estimators': 209, 'max_depth': 9, 'learning_rate': 0.22273828101892074, 'subsample': 0.6683338890343442, 'colsample_bytree': 0.6128574633569317, 'gamma': 0.5466845487515618, 'reg_alpha': 0.6431344061282336, 'reg_lambda': 4.203225113117078, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0015, R²: 0.3787\n",
      "CPU Usage - Test  RMSE: 0.0012, R²: 0.1652\n",
      "Best Hyperparameters: {'n_estimators': 167, 'max_depth': 3, 'learning_rate': 0.17611976873731136, 'subsample': 0.88407194521103, 'colsample_bytree': 0.6917786085819043, 'gamma': 3.1772844129089726, 'reg_alpha': 0.29867268420588633, 'reg_lambda': 2.9177094798634187, 'random_state': 42}\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 4.8802, R²: 0.4528\n",
      "Memory Usage - Test  RMSE: 8.6175, R²: -0.0561\n",
      "Best Hyperparameters: {'n_estimators': 140, 'max_depth': 5, 'learning_rate': 0.21314808313598121, 'subsample': 0.602827696246445, 'colsample_bytree': 0.6864146328418634, 'gamma': 4.984480283097849, 'reg_alpha': 2.553589847629622, 'reg_lambda': 1.1044195303546593, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0014, R²: 0.3412\n",
      "CPU Usage - Test  RMSE: 0.0013, R²: 0.1567\n",
      "Best Hyperparameters: {'n_estimators': 156, 'max_depth': 5, 'learning_rate': 0.2996543859524117, 'subsample': 0.971239000599236, 'colsample_bytree': 0.7118657531927448, 'gamma': 4.871584978090299, 'reg_alpha': 0.003264938122049088, 'reg_lambda': 0.029663175594696944, 'random_state': 42}\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 4.6373, R²: 0.4750\n",
      "Memory Usage - Test  RMSE: 10.3494, R²: -0.3184\n",
      "Best Hyperparameters: {'n_estimators': 122, 'max_depth': 8, 'learning_rate': 0.042230946343293815, 'subsample': 0.6190924173259228, 'colsample_bytree': 0.780470813798125, 'gamma': 4.599685393895794, 'reg_alpha': 3.902907849173288, 'reg_lambda': 0.2565958487956952, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0014, R²: 0.3736\n",
      "CPU Usage - Test  RMSE: 0.0012, R²: 0.0115\n",
      "Best Hyperparameters: {'n_estimators': 236, 'max_depth': 8, 'learning_rate': 0.277093204388894, 'subsample': 0.9795192443233156, 'colsample_bytree': 0.7034357316960176, 'gamma': 2.118909566079694, 'reg_alpha': 2.5595671406092007, 'reg_lambda': 4.7186836607947615, 'random_state': 42}\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 2.6116, R²: 0.8763\n",
      "Memory Usage - Test  RMSE: 3.4643, R²: -0.2799\n",
      "Best Hyperparameters: {'n_estimators': 202, 'max_depth': 4, 'learning_rate': 0.13559926101254785, 'subsample': 0.8260519907243399, 'colsample_bytree': 0.8490680384322359, 'gamma': 0.07717676588551947, 'reg_alpha': 2.766261223910716, 'reg_lambda': 4.497415539188959, 'random_state': 42}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, df in configs.items():\n",
    "    for test_size in test_sizes:\n",
    "        print(f\"Training XGBoost for {name} - CPU Usage with test size {test_size}\")\n",
    "        model_cpu = train_xgboost_model_optuna(df, \"CPU Usage\", test_size)\n",
    "\n",
    "        print(f\"Training XGBoost for {name} - Memory Usage with test size {test_size}\")\n",
    "        model_mem = train_xgboost_model_optuna(df, \"Memory Usage\", test_size)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b10516cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_features(df, window=3):\n",
    "    df = df.copy()\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='mixed')\n",
    "\n",
    "    df = df.sort_values(['Service', 'Timestamp'])  # Service-wise time sorting\n",
    "    df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "    # Rolling averages per service\n",
    "    for col in ['CPU Usage', 'Memory Usage', 'Latency']:\n",
    "        df[f'{col}_RollingMean'] = df.groupby('Service')[col].transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "        df[f'{col}_RollingSTD'] = df.groupby('Service')[col].transform(lambda x: x.rolling(window, min_periods=1).std())\n",
    "\n",
    "    # Spike detection\n",
    "    df[\"CPU_Spike\"] = df[\"CPU Usage\"] - df[\"CPU Usage_RollingMean\"]\n",
    "    df[\"Memory_Spike\"] = df[\"Memory Usage\"] - df[\"Memory Usage_RollingMean\"]\n",
    "\n",
    "    # Latency trend direction\n",
    "    df[\"Latency_Trend\"] = df.groupby(\"Service\")[\"Latency\"].transform(lambda x: x.diff().fillna(0).apply(lambda y: 1 if y > 0 else (-1 if y < 0 else 0)))\n",
    "\n",
    "    df.reset_index(inplace=True)  # Reset index to include Timestamp again\n",
    "    df.dropna(inplace=True)  # Optional: drop rows with NaNs from rolling\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee103217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model_new_features(df, feature_col, test_size=0.2, plot=True):\n",
    "    df = add_rolling_features(df)\n",
    "    df = df.sort_values(\"Timestamp\")\n",
    "    df = df[[feature_col, \"CPU Request\", \"Memory Request\", \"CPU Limit\", \"Memory Limit\", \"Latency\",\n",
    "    \"CPU Usage_RollingMean\", \"Memory Usage_RollingMean\", \"Latency_RollingMean\",\n",
    "    \"CPU Usage_RollingSTD\", \"Memory Usage_RollingSTD\", \"Latency_RollingSTD\",\n",
    "    \"CPU_Spike\", \"Memory_Spike\", \"Latency_Trend\"]].dropna()\n",
    "\n",
    "    features = [\n",
    "    \"CPU Request\", \"Memory Request\", \"CPU Limit\", \"Memory Limit\", \"Latency\",\n",
    "    \"CPU Usage_RollingMean\", \"Memory Usage_RollingMean\", \"Latency_RollingMean\",\n",
    "    \"CPU Usage_RollingSTD\", \"Memory Usage_RollingSTD\", \"Latency_RollingSTD\",\n",
    "    \"CPU_Spike\", \"Memory_Spike\", \"Latency_Trend\"\n",
    "    ]\n",
    "\n",
    "    target = feature_col\n",
    "\n",
    "    if \"Memory\" in feature_col:\n",
    "        df[feature_col] = df[feature_col] / (1024 * 1024)\n",
    "\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    X_scaled = feature_scaler.fit_transform(df[features])\n",
    "\n",
    "    target_scaler = MinMaxScaler()\n",
    "    y_scaled = target_scaler.fit_transform(df[[target]]).ravel()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=test_size, shuffle=False)\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 300),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 5),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 5),\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(X_train, y_train, verbose=False)\n",
    "        preds = model.predict(X_test)\n",
    "        rmse = mean_squared_error(y_test, preds)\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=50, show_progress_bar=False)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_params[\"random_state\"] = 42\n",
    "    model = XGBRegressor(**best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "\n",
    "    pred_train_inv = target_scaler.inverse_transform(pred_train.reshape(-1, 1)).ravel()\n",
    "    y_train_inv = target_scaler.inverse_transform(y_train.reshape(-1, 1)).ravel()\n",
    "    pred_test_inv = target_scaler.inverse_transform(pred_test.reshape(-1, 1)).ravel()\n",
    "    y_test_inv = target_scaler.inverse_transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_inv, pred_train_inv))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_inv, pred_test_inv))\n",
    "    train_r2 = r2_score(y_train_inv, pred_train_inv)\n",
    "    test_r2 = r2_score(y_test_inv, pred_test_inv)\n",
    "\n",
    "    print(f\"{feature_col} - Train RMSE: {train_rmse:.4f}, R²: {train_r2:.4f}\")\n",
    "    print(f\"{feature_col} - Test  RMSE: {test_rmse:.4f}, R²: {test_r2:.4f}\")\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "    # if plot:\n",
    "    #     plt.figure(figsize=(10, 4))\n",
    "    #     plt.plot(y_test_inv, label=\"Actual\")\n",
    "    #     plt.plot(pred_test_inv, label=\"Predicted\")\n",
    "    #     plt.title(f\"{feature_col} Prediction (XGBoost + Optuna)\")\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5dd76b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost for Service 1 - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0013, R²: 0.9981\n",
      "CPU Usage - Test  RMSE: 0.0014, R²: 0.9973\n",
      "Best Hyperparameters: {'n_estimators': 185, 'max_depth': 5, 'learning_rate': 0.29058368439756266, 'subsample': 0.9556875816167616, 'colsample_bytree': 0.9119056367951991, 'gamma': 0.015652571329133252, 'reg_alpha': 0.4480593787950613, 'reg_lambda': 1.6482541254779657, 'random_state': 42}\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 5.0474, R²: 0.7798\n",
      "Memory Usage - Test  RMSE: 5.4257, R²: 0.3131\n",
      "Best Hyperparameters: {'n_estimators': 121, 'max_depth': 8, 'learning_rate': 0.16268645355411424, 'subsample': 0.6201741512013198, 'colsample_bytree': 0.9798480298638017, 'gamma': 0.000370898367562187, 'reg_alpha': 4.4275992521611585, 'reg_lambda': 2.9615841131031115, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 1 - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0028, R²: 0.9896\n",
      "CPU Usage - Test  RMSE: 0.0032, R²: 0.9687\n",
      "Best Hyperparameters: {'n_estimators': 245, 'max_depth': 9, 'learning_rate': 0.18866940412507688, 'subsample': 0.7657856954982643, 'colsample_bytree': 0.8840311917419648, 'gamma': 0.025474660704945605, 'reg_alpha': 4.464385170743855, 'reg_lambda': 3.2941443098387437, 'random_state': 42}\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 1.9032, R²: 0.9649\n",
      "Memory Usage - Test  RMSE: 6.3654, R²: 0.2653\n",
      "Best Hyperparameters: {'n_estimators': 222, 'max_depth': 10, 'learning_rate': 0.04752224652603009, 'subsample': 0.8854365766156802, 'colsample_bytree': 0.6265630271204601, 'gamma': 0.04237903421541468, 'reg_alpha': 0.31652914840004154, 'reg_lambda': 3.8275457800132435, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 1 - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0032, R²: 0.9877\n",
      "CPU Usage - Test  RMSE: 0.0026, R²: 0.9176\n",
      "Best Hyperparameters: {'n_estimators': 201, 'max_depth': 11, 'learning_rate': 0.1406094154897604, 'subsample': 0.6366398930180055, 'colsample_bytree': 0.6742283112058889, 'gamma': 0.13069019863153175, 'reg_alpha': 2.5127756211474974, 'reg_lambda': 3.7671975492607537, 'random_state': 42}\n",
      "Training XGBoost for Service 1 - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 2.6784, R²: 0.9244\n",
      "Memory Usage - Test  RMSE: 8.8483, R²: 0.1685\n",
      "Best Hyperparameters: {'n_estimators': 147, 'max_depth': 9, 'learning_rate': 0.29231103411210624, 'subsample': 0.930926233097827, 'colsample_bytree': 0.6733346172093727, 'gamma': 0.023564058274890144, 'reg_alpha': 1.4757392222811254, 'reg_lambda': 4.060412982688144, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0008, R²: 0.9992\n",
      "CPU Usage - Test  RMSE: 0.0017, R²: 0.9940\n",
      "Best Hyperparameters: {'n_estimators': 134, 'max_depth': 9, 'learning_rate': 0.1537394815761155, 'subsample': 0.7395753818996681, 'colsample_bytree': 0.7518035892485757, 'gamma': 0.002108010612546639, 'reg_alpha': 1.676468901771744, 'reg_lambda': 2.7417832138270013, 'random_state': 42}\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 0.0375, R²: 0.9810\n",
      "Memory Usage - Test  RMSE: 0.0456, R²: 0.9664\n",
      "Best Hyperparameters: {'n_estimators': 173, 'max_depth': 3, 'learning_rate': 0.23589922340146924, 'subsample': 0.6930857274468333, 'colsample_bytree': 0.9414641300868157, 'gamma': 0.018960348889535125, 'reg_alpha': 1.2251075547879937, 'reg_lambda': 0.20462643628776112, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0004, R²: 0.9997\n",
      "CPU Usage - Test  RMSE: 0.0005, R²: 0.9990\n",
      "Best Hyperparameters: {'n_estimators': 112, 'max_depth': 3, 'learning_rate': 0.16072262225260436, 'subsample': 0.9443372408412379, 'colsample_bytree': 0.9619377558085805, 'gamma': 0.003649826609564099, 'reg_alpha': 0.33448297717428427, 'reg_lambda': 1.7650583527914723, 'random_state': 42}\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 0.0536, R²: 0.9618\n",
      "Memory Usage - Test  RMSE: 0.0578, R²: 0.9442\n",
      "Best Hyperparameters: {'n_estimators': 194, 'max_depth': 5, 'learning_rate': 0.1577402912460662, 'subsample': 0.8025246923939008, 'colsample_bytree': 0.9623325592960241, 'gamma': 0.031068398090356896, 'reg_alpha': 3.9651198250206328, 'reg_lambda': 0.8945449351445223, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for Service 2 - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0006, R²: 0.9996\n",
      "CPU Usage - Test  RMSE: 0.0009, R²: 0.9852\n",
      "Best Hyperparameters: {'n_estimators': 206, 'max_depth': 5, 'learning_rate': 0.2000364332659984, 'subsample': 0.8561467906384783, 'colsample_bytree': 0.9488701910657633, 'gamma': 0.007254262449723825, 'reg_alpha': 1.683586516473337, 'reg_lambda': 0.014223323361770956, 'random_state': 42}\n",
      "Training XGBoost for Service 2 - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 0.0492, R²: 0.9677\n",
      "Memory Usage - Test  RMSE: 0.0394, R²: 0.9670\n",
      "Best Hyperparameters: {'n_estimators': 237, 'max_depth': 9, 'learning_rate': 0.22044489412469526, 'subsample': 0.9451973826655812, 'colsample_bytree': 0.9444055562701238, 'gamma': 0.24006681752808134, 'reg_alpha': 0.7593364774200809, 'reg_lambda': 2.017461198464283, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0003, R²: 0.9988\n",
      "CPU Usage - Test  RMSE: 0.0012, R²: 0.9908\n",
      "Best Hyperparameters: {'n_estimators': 168, 'max_depth': 12, 'learning_rate': 0.09912238712677796, 'subsample': 0.6035264628515163, 'colsample_bytree': 0.8643324630375584, 'gamma': 0.03426279421321787, 'reg_alpha': 0.05773442054637101, 'reg_lambda': 1.3800839265519769, 'random_state': 42}\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 1.3428, R²: 0.9946\n",
      "Memory Usage - Test  RMSE: 4.1776, R²: 0.9687\n",
      "Best Hyperparameters: {'n_estimators': 276, 'max_depth': 7, 'learning_rate': 0.01591954010143272, 'subsample': 0.7429019736647035, 'colsample_bytree': 0.796808532577829, 'gamma': 0.0012041978706387423, 'reg_alpha': 0.22149703980833593, 'reg_lambda': 1.6617967118752455, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0004, R²: 0.9985\n",
      "CPU Usage - Test  RMSE: 0.0015, R²: 0.9868\n",
      "Best Hyperparameters: {'n_estimators': 147, 'max_depth': 10, 'learning_rate': 0.14120921636592676, 'subsample': 0.7984762269364442, 'colsample_bytree': 0.728227211057545, 'gamma': 0.0006752328455172929, 'reg_alpha': 2.2248736516171657, 'reg_lambda': 2.344066797949029, 'random_state': 42}\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 1.9863, R²: 0.9874\n",
      "Memory Usage - Test  RMSE: 3.9620, R²: 0.9783\n",
      "Best Hyperparameters: {'n_estimators': 182, 'max_depth': 3, 'learning_rate': 0.022888735287841924, 'subsample': 0.9965514723864294, 'colsample_bytree': 0.8447083293504782, 'gamma': 0.05127053951159447, 'reg_alpha': 0.04748557335502417, 'reg_lambda': 1.171414977699757, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for HashGen - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0006, R²: 0.9964\n",
      "CPU Usage - Test  RMSE: 0.0017, R²: 0.9755\n",
      "Best Hyperparameters: {'n_estimators': 109, 'max_depth': 4, 'learning_rate': 0.15123696179994023, 'subsample': 0.8253193954621352, 'colsample_bytree': 0.9487764204559487, 'gamma': 0.22450429949855694, 'reg_alpha': 0.4902547191462374, 'reg_lambda': 4.686981572494532, 'random_state': 42}\n",
      "Training XGBoost for HashGen - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 1.6865, R²: 0.9900\n",
      "Memory Usage - Test  RMSE: 5.5585, R²: 0.9738\n",
      "Best Hyperparameters: {'n_estimators': 272, 'max_depth': 12, 'learning_rate': 0.07044185679147033, 'subsample': 0.8533098980639727, 'colsample_bytree': 0.9967916804744851, 'gamma': 0.05381318590738541, 'reg_alpha': 0.04924884379569547, 'reg_lambda': 0.9337009769787084, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.3\n",
      "CPU Usage - Train RMSE: 0.0002, R²: 0.9931\n",
      "CPU Usage - Test  RMSE: 0.0002, R²: 0.9828\n",
      "Best Hyperparameters: {'n_estimators': 129, 'max_depth': 8, 'learning_rate': 0.12168681722445882, 'subsample': 0.9353337716404994, 'colsample_bytree': 0.8814337401573011, 'gamma': 0.007489582745024567, 'reg_alpha': 2.191244506240067, 'reg_lambda': 0.008330328167377998, 'random_state': 42}\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.3\n",
      "Memory Usage - Train RMSE: 0.9406, R²: 0.9796\n",
      "Memory Usage - Test  RMSE: 1.3072, R²: 0.9757\n",
      "Best Hyperparameters: {'n_estimators': 184, 'max_depth': 11, 'learning_rate': 0.08225920044774401, 'subsample': 0.819697470454124, 'colsample_bytree': 0.9446975330445966, 'gamma': 0.36119318181619275, 'reg_alpha': 0.6191240312096429, 'reg_lambda': 3.814805177118314, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.2\n",
      "CPU Usage - Train RMSE: 0.0002, R²: 0.9918\n",
      "CPU Usage - Test  RMSE: 0.0001, R²: 0.9890\n",
      "Best Hyperparameters: {'n_estimators': 202, 'max_depth': 6, 'learning_rate': 0.2768733498429673, 'subsample': 0.9814240464846508, 'colsample_bytree': 0.9102119182145338, 'gamma': 0.0007249243643192849, 'reg_alpha': 3.6092325574730246, 'reg_lambda': 2.124945395919719, 'random_state': 42}\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.2\n",
      "Memory Usage - Train RMSE: 0.9027, R²: 0.9801\n",
      "Memory Usage - Test  RMSE: 1.3764, R²: 0.9767\n",
      "Best Hyperparameters: {'n_estimators': 246, 'max_depth': 8, 'learning_rate': 0.19295712889199093, 'subsample': 0.8280701526989376, 'colsample_bytree': 0.9700011753364561, 'gamma': 0.2860218280548321, 'reg_alpha': 2.144247649756337, 'reg_lambda': 3.917454468733232, 'random_state': 42}\n",
      "\n",
      "Training XGBoost for RandPw - CPU Usage with test size 0.1\n",
      "CPU Usage - Train RMSE: 0.0002, R²: 0.9913\n",
      "CPU Usage - Test  RMSE: 0.0001, R²: 0.9899\n",
      "Best Hyperparameters: {'n_estimators': 143, 'max_depth': 11, 'learning_rate': 0.04670712054068754, 'subsample': 0.9467389876221157, 'colsample_bytree': 0.9422652684388263, 'gamma': 0.009704073889516218, 'reg_alpha': 3.22517243483581, 'reg_lambda': 1.1427098628786483, 'random_state': 42}\n",
      "Training XGBoost for RandPw - Memory Usage with test size 0.1\n",
      "Memory Usage - Train RMSE: 0.5154, R²: 0.9952\n",
      "Memory Usage - Test  RMSE: 0.2591, R²: 0.9928\n",
      "Best Hyperparameters: {'n_estimators': 202, 'max_depth': 4, 'learning_rate': 0.12755298576169644, 'subsample': 0.9688208688560752, 'colsample_bytree': 0.9734738894021356, 'gamma': 0.00026705944615321164, 'reg_alpha': 2.82120610154872, 'reg_lambda': 0.20230831210107197, 'random_state': 42}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, df in configs.items():\n",
    "    for test_size in test_sizes:\n",
    "        print(f\"Training XGBoost for {name} - CPU Usage with test size {test_size}\")\n",
    "        model_cpu = train_xgboost_model_new_features(df, \"CPU Usage\", test_size)\n",
    "\n",
    "        print(f\"Training XGBoost for {name} - Memory Usage with test size {test_size}\")\n",
    "        model_mem = train_xgboost_model_new_features(df, \"Memory Usage\", test_size)\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
